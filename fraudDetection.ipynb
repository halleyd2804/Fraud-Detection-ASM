{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01fdc6f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "669d0dfcff5e3b435efc1a92190a690a",
     "grade": false,
     "grade_id": "cell-b06d2fac0e18b03b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<span class=\"alert alert-success\" style=\"display: block;\">\n",
    "    \n",
    "## FNCE30012: Foundations of Fintech - Assignment 1: Fraud Detection\n",
    "\n",
    "</span>\n",
    "\n",
    "<span class=\"alert alert-info\" style=\"display: inline-block;\">\n",
    "All questions must be completed and submitted via Jupyter Hub (any other submissions will not be\n",
    "marked). The assignment notebook contains certain pre-defined variables. A submission that renames these\n",
    "variables will attract zero marks. Calculations involving decimal numbers should be precise till 3 decimal\n",
    "places (e.g., if answer is 4.122229, then one should use 4.123 instead of 4.1). \n",
    "\n",
    "You are **NOT** allowed to use `sleep` or other methods to slow down execution. \n",
    "    \n",
    "Please use the **ED** forum to ask any queries regarding this assignment. \n",
    "\n",
    "<hr/>\n",
    "    \n",
    "**Late submission penalties**\n",
    "- 10% of the maximum mark per day the assessment task is overdue;\n",
    "- Assignments submitted 10 days after the deadline will not be graded and will receive a grade of zero.\n",
    "- Exceeding word limits: Assessment tasks should not vary more than 10% from the required word count (where relevant).\n",
    "\n",
    "**Assignment Extensions**\n",
    "- Faculty extensions policy can be found here: [http://policy.unimelb.edu.au/MPF1326#section-4.37](http://policy.unimelb.edu.au/MPF1326#section-4.37)\n",
    "- Requests for an assignment extension should be submitted here: [http://go.unimelb.edu.au/yh9n](http://go.unimelb.edu.au/yh9n)\n",
    "\n",
    "**Submission**\n",
    "This assignment should be submitted via the JupyterHub system. \n",
    "\n",
    "**Plagiarism declaration**\n",
    "By submitting work for assessment I hereby declare that I understand the Universityâ€™s policy on academic integrity and statement on the use of artificial intelligence software. In accordance with these documents, I declare that the work submitted is original and solely my work, and that I have not been assisted by a third party (collusion) apart from where the submitted work is for a designated collaborative task, in which case the individual contributions are indicated. I also declare that I have not used any writing tools or sources without proper acknowledgement (plagiarism). Where the submitted work is a computer program or code, I further declare that any copied code is declared in comments identifying the source at the start of the program or in a header file, that comments inline identify the start and end of the copied code, and that any modifications to code sources elsewhere are commented upon as to the nature of the modification.\n",
    "    \n",
    "<hr/>\n",
    "    \n",
    "**Important:** It is important that you follow the assignment submission instructions as per \"How to submit assignments on JupyterHub\" available under Modules/Assessment on Canvas. \n",
    "\n",
    "<span class=\"label label-danger text-uppercase\">Note</span>: The assignment will ask you to follow certain precise instructions such as storing results in predefined variable names, or reading CSV files into dataframes with specific names. Failure to follow these instructions would attract penalty.\n",
    "</span>\n",
    "\n",
    "<span class=\"alert alert-danger\"  style=\"display: inline-block;\">\n",
    "    <span class=\"label label-danger text-uppercase\">Note</span>\n",
    "    <span>\n",
    "        Write your answers in <strong>this notebook</strong> only <em>within</em> the provided cells. <em>Do not</em> create additional cells or duplicate/copy existing cells. <em>Do not</em> rename this file. <br/>\n",
    "        You may keep a copy of this base notebook as reference to original file and to compare that you haven't accidentally added cells, duplicated cells or changed the order of these cells. However, note that additional notebooks and copies aren't marked. \n",
    "    </span>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7597d958",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96d4d3feadd9b4db0293c35cdfbc7edd",
     "grade": false,
     "grade_id": "cell-386d0d1dfb83796d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<span class=\"alert alert-info\" style=\"display: inline-block;\">\n",
    "\n",
    "**Associated Data Files:** We will use two csv files for this assignment:\n",
    "1. `transactions.csv` Each row denotes a transaction. The columns are as follows:\n",
    "\n",
    "| Column Name | Description |\n",
    "|:-----|:-----|\n",
    "| `TRANSACTION_ID` | A unique ID of the credit card transaction |\n",
    "| `TX_DATETIME`  | Date time of the transaction |\n",
    "| `CUSTOMER_ID` | ID of the credit card owner |\n",
    "| `TERMINAL_ID`  | ID of the terminal where transaction originated |\n",
    "| `TX_AMOUNT` | Amount of the transaction in dollars |\n",
    "| `TX_FRAUD` | Label denoting if the transaction was fraudulent (label of 1) or genuine (label of 0) |\n",
    "\n",
    "2. `card_data.csv`. Each row denotes a transaction. This file contains pre-computed feature engineered columns (in addition to the ones listed above). The extra columns are as follows:\n",
    "\n",
    "| Column Name | Description |\n",
    "|---|---|\n",
    "| `IS_WEEKEND` | Denotes if the transaction occurred over a weekend (label of 1) or a weekday (label of 0) |\n",
    "| `IS_WEDNESDAY` | Denotes if the transaction occurred on Wednesday (label of 1) or any other day of the week (label of 0) |\n",
    "| `IS_NIGHT` | Denotes if the transaction occurred during the night (label of 1) or during the day (label of 0) |\n",
    "| `TX_HOUR` | The hour at which the transaction occurred |\n",
    "| `TX_DATE` | The date of the transaction (i.e., the date excluding the time information) |\n",
    "| `TX_WEEK`  | The week of the transaction (i.e., the week number when the transaction occurred) |\n",
    "| `TERMINAL_TOTAL_1D` | The number of transactions that occurred in the previous day for the associated terminal ID |\n",
    "| `TERMINAL_FRAUD_1D` | The number of fraudulent transactions that occurred in the previous day for the associated terminal ID |\n",
    "| `TERMINAL_RISK_1D` | The proportion of transactions that were fraudulent in the previous day for the associated terminal ID |\n",
    "| `CUSTOMER_TOTAL_1D` | The number of transactions that occurred in the previous day for the associated customer ID |\n",
    "| `CUSTOMER_FRAUD_1D` | The number of fraudulent transactions that occurred in the previous day for the associated customer ID |\n",
    "| `CUSTOMER_RISK_1D` | The proportion of transactions that were fraudulent in the previous day for the associated customer ID |\n",
    "| `TERMINAL_TOTAL_1W`  | The number of transactions that occurred in the previous week for the associated terminal ID |\n",
    "| `TERMINAL_FRAUD_1W`  | The number of fraudulent transactions that occurred in the previous week for the associated terminal ID |\n",
    "| `TERMINAL_RISK_1W`  | The proportion of transactions that were fraudulent in the previous week for the associated terminal ID |\n",
    "| `CUSTOMER_TOTAL_1W` | The number of transactions that occurred in the previous week for the associated customer ID |\n",
    "| `CUSTOMER_FRAUD_1W` | The number of fraudulent transactions that occurred in the previous week for the associated customer ID |\n",
    "| `CUSTOMER_RISK_1W`  | The proportion of transactions that were fraudulent in the previous week for the associated customer ID |\n",
    "| `SPENT_1D`  | The average spent for the customer in the previous day |\n",
    "| `SPENT_1W` | The average spent for the customer in the previous week |\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3fddd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa0ffd38e8a245e58c395ce4de3bcf9a",
     "grade": false,
     "grade_id": "cell-b3f6d0fe5e4d44cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Predefined Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c95aae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T10:19:16.643686Z",
     "start_time": "2024-08-31T10:19:10.150771Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e86bd34afcf0284cc5b50acc399f7a4",
     "grade": false,
     "grade_id": "cell-679014dc5257cb7e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Import Required Libraries - No need to duplicate or reimport; \n",
    "This is a Read-Only cell. Remember to execute this cell once\"\"\"\n",
    "\n",
    "# Import the numpy and pandas library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for date time\n",
    "from datetime import datetime\n",
    "\n",
    "# Import the matplotlib library, used for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# for stats and ML\n",
    "import statsmodels.api as sm\n",
    "from finml import train_model, build_model, split_by_threshold, evaluate, similarity, tfidf_vectorize\n",
    "from sklearn import metrics\n",
    "\n",
    "# disable messages to increase output readability\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import random as tf_random\n",
    "from tensorflow.keras.backend import clear_session\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5272b8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T10:19:16.647962Z",
     "start_time": "2024-08-31T10:19:16.645169Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47a438d060d0d7de19c2a1fac3544503",
     "grade": false,
     "grade_id": "cell-b2dc3a3db960b6d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"This is a Read-Only cell. Remember to execute this cell once\"\"\"\n",
    "def set_seeds(seed=123):\n",
    "    import os, random\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69888bc1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0eaf7cae0493b1d5687cf00077da0d78",
     "grade": false,
     "grade_id": "cell-93ed5aa902c232bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Import your additional packages in the cell below\n",
    "Further imports are not required, but you may use additional packages if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "034d95ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T10:19:16.964739Z",
     "start_time": "2024-08-31T10:19:16.649041Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4972029d34aa63e37edb1ea369a4899",
     "grade": true,
     "grade_id": "cell-5fef749622614bcb",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Your own additional imports go here\"\"\"\n",
    "# BEGIN - YOUR CODE GOES HERE\n",
    "import calendar\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# END - YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1849fb7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T10:19:17.148204Z",
     "start_time": "2024-08-31T10:19:16.966658Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73ff260ef0085b2a46048a0b24bfed68",
     "grade": false,
     "grade_id": "cell-d934074fc740b14c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TX_DATETIME</th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>TERMINAL_ID</th>\n",
       "      <th>TX_AMOUNT</th>\n",
       "      <th>TX_FRAUD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRANSACTION_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>730399</th>\n",
       "      <td>2024-03-02 16:58:04</td>\n",
       "      <td>1610</td>\n",
       "      <td>7393</td>\n",
       "      <td>122.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718607</th>\n",
       "      <td>2024-03-02 03:54:17</td>\n",
       "      <td>1610</td>\n",
       "      <td>3193</td>\n",
       "      <td>96.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732845</th>\n",
       "      <td>2024-03-02 22:20:50</td>\n",
       "      <td>1610</td>\n",
       "      <td>2239</td>\n",
       "      <td>167.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726608</th>\n",
       "      <td>2024-03-02 12:59:55</td>\n",
       "      <td>1610</td>\n",
       "      <td>9572</td>\n",
       "      <td>121.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729587</th>\n",
       "      <td>2024-03-02 15:54:29</td>\n",
       "      <td>1610</td>\n",
       "      <td>9850</td>\n",
       "      <td>191.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TX_DATETIME  CUSTOMER_ID  TERMINAL_ID  TX_AMOUNT  \\\n",
       "TRANSACTION_ID                                                            \n",
       "730399         2024-03-02 16:58:04         1610         7393     122.82   \n",
       "718607         2024-03-02 03:54:17         1610         3193      96.39   \n",
       "732845         2024-03-02 22:20:50         1610         2239     167.10   \n",
       "726608         2024-03-02 12:59:55         1610         9572     121.51   \n",
       "729587         2024-03-02 15:54:29         1610         9850     191.29   \n",
       "\n",
       "                TX_FRAUD  \n",
       "TRANSACTION_ID            \n",
       "730399                 0  \n",
       "718607                 0  \n",
       "732845                 0  \n",
       "726608                 0  \n",
       "729587                 0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Use the preloaded data in your workings below;\n",
    "This is a Read-Only cell. Remember to execute this cell once\"\"\"\n",
    "df_transactions = pd.read_csv(\"./transactions.csv\", index_col=\"TRANSACTION_ID\", parse_dates=[\"TX_DATETIME\"])\n",
    "df_transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42409d23",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74c981a4d314e0e15a835403d6efd0c5",
     "grade": false,
     "grade_id": "cell-278d6e1898dd0d49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 1 [1 marks]\n",
    "\n",
    "<strong> Q1 (i) [<font color=\"red\"> 0.5 marks </font>]</strong> \n",
    "The data contains transactions from `2024-03-01` till `2024-03-10` (inclusive). You should use all data till `2024-03-06` (inclusive) for training, and transactions that happened on `2024-03-10` for testing.\n",
    "\n",
    "Split the given transaction data into training and testing sets based on these dates. You should use the provided variables to store the split data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f225a24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T10:19:17.151004Z",
     "start_time": "2024-08-31T10:19:17.149190Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ba9b8c5239c2cd885d7dcc010c3e82e",
     "grade": false,
     "grade_id": "cell-87be5339f24f7052",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Predefined Variables - Do Not Change their Name;\n",
    "This is a Read-Only cell. Remember to execute this cell once\"\"\"\n",
    "df_train = None  # Replace None with correct value/code\n",
    "df_test = None  # Replace None with correct value/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d77a1bb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T10:19:17.329419Z",
     "start_time": "2024-08-31T10:19:17.151873Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e168cc9556858e28469a80c8a88a49f0",
     "grade": true,
     "grade_id": "cell-3119794aecbfd36f",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Populate the variables shown above with appropriate values here\"\"\"\n",
    "# BEGIN - YOUR CODE GOES HERE\n",
    "df_train = df_transactions.loc[df_transactions['TX_DATETIME'].dt.date <= pd.to_datetime(\"2024-03-06\")]\n",
    "df_test = df_transactions.loc[df_transactions['TX_DATETIME'].dt.date == pd.to_datetime(\"2024-03-10\")]\n",
    "# END - YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f8f85",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64897a521c63df3a34e148131557f496",
     "grade": false,
     "grade_id": "cell-0215ac6f4e69c1c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong> Q1 (ii) [<font color=\"red\"> 0.5 marks </font>]</strong> Plot the histogram of fraudulent and genuine transactions for the training and test datasets. You should plot them side by side with the plot for the training dataset on the left side of the plot of the test dataset. Your plots should be appropriately labelled. You should use a log scale for the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52207bd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T11:33:26.796006Z",
     "start_time": "2024-08-31T11:33:26.482937Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db479062b3946669df2492c3eaa703e6",
     "grade": true,
     "grade_id": "cell-1d09c281de26b61a",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BEGIN - YOUR CODE GOES HERE\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "# Histogram for the training dataset\n",
    "sns.histplot(data=df_train, x=\"TX_FRAUD\", discrete=True, ax=ax1, log_scale=(False, True))\n",
    "ax1.set_title(\"Training Dataset\")\n",
    "ax1.set_xlabel(\"TX_FRAUD\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.set_xticks([0, 1])\n",
    "ax1.set_xticklabels(['Genuine', 'Fraudulent'])\n",
    "\n",
    "# Histogram for the test dataset\n",
    "sns.histplot(data=df_test, x=\"TX_FRAUD\", discrete=True, ax=ax2, log_scale=(False, True))\n",
    "ax2.set_title(\"Testing Dataset\")\n",
    "ax2.set_xlabel(\"TX_FRAUD\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.set_xticks([0, 1])\n",
    "ax2.set_xticklabels(['Genuine', 'Fraudulent'])\n",
    "plt.show()\n",
    "# END - YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e49c1b9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "443d7bccdd01e5bfb2c3c5a41888c11b",
     "grade": false,
     "grade_id": "cell-45dd2b7dc64b8675",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 2 [2 marks]\n",
    "\n",
    "<strong> Q2 (i) [<font color=\"red\"> 0.5 marks </font>]</strong> Fit a logistic regression model using the **relevant features** from the training portion of the transactions data.  You should use the given variable to store the fitted logistic regression model. You should also use the `statsmodels` package for logistic regression.\n",
    "\n",
    "Note: You should not generate new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "922e6a22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:27:10.030372Z",
     "start_time": "2024-08-28T13:27:10.028279Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2da46207e97e7471c08b975a363e472",
     "grade": false,
     "grade_id": "cell-e0cde20eb2971aee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Predefined Variables - Do Not Change their Name;\n",
    "This is a Read-Only cell. Remember to execute this cell once\"\"\"\n",
    "logit_1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d1f6089",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:27:10.379690Z",
     "start_time": "2024-08-28T13:27:10.031303Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9524665e308fa253dc807973caea6d58",
     "grade": true,
     "grade_id": "cell-861ffa2225aea86a",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.026017\n",
      "         Iterations 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>TX_FRAUD</td>     <th>  No. Observations:  </th>   <td> 72687</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td> 72685</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     1</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 28 Aug 2024</td> <th>  Pseudo R-squ.:     </th>   <td>0.1104</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>23:27:10</td>     <th>  Log-Likelihood:    </th>  <td> -1891.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -2125.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>4.713e-104</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>     <td>   -6.5558</td> <td>    0.094</td> <td>  -70.022</td> <td> 0.000</td> <td>   -6.739</td> <td>   -6.372</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TX_AMOUNT</th> <td>    0.0067</td> <td>    0.000</td> <td>   20.481</td> <td> 0.000</td> <td>    0.006</td> <td>    0.007</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:               TX_FRAUD   No. Observations:                72687\n",
       "Model:                          Logit   Df Residuals:                    72685\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Wed, 28 Aug 2024   Pseudo R-squ.:                  0.1104\n",
       "Time:                        23:27:10   Log-Likelihood:                -1891.1\n",
       "converged:                       True   LL-Null:                       -2125.7\n",
       "Covariance Type:            nonrobust   LLR p-value:                4.713e-104\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -6.5558      0.094    -70.022      0.000      -6.739      -6.372\n",
       "TX_AMOUNT      0.0067      0.000     20.481      0.000       0.006       0.007\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Populate the variables shown above with appropriate values here\"\"\"\n",
    "# BEGIN - YOUR CODE GOES HERE\n",
    "features = ['TX_AMOUNT']\n",
    "predict = \"TX_FRAUD\"\n",
    "\n",
    "x = df_train[features] \n",
    "x = sm.add_constant(x)\n",
    "y = df_train[predict]\n",
    "\n",
    "logit_1 = sm.Logit(y,x).fit()\n",
    "logit_1.summary()\n",
    "# END - YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84546d4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:27:10.473287Z",
     "start_time": "2024-08-28T13:27:10.381334Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f98926932512337813f92424e83a6721",
     "grade": false,
     "grade_id": "cell-135c851c478b9958",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"This is a Read-Only cell. Remember to execute this cell once\"\"\"\n",
    "try:\n",
    "    logit_1.summary()\n",
    "except NameError:\n",
    "    print(f\"Did you forget to run the readonly cell above?\")\n",
    "except AttributeError:\n",
    "    print(f\"Your code is possibly incorrect for creating logit_1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688af992",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59045e1533c96020980d7d0101fa5b57",
     "grade": false,
     "grade_id": "cell-c0aca25a569de87d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong> Q2 (ii) [<font color=\"red\"> 0.5 marks </font>]</strong> Train a neural network model using the **relevant features** from the training portiion of the transactions data. You should use the given variables to store the model components. The neural network should have 2 hidden layers with 8 units each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5dd995e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:27:10.491891Z",
     "start_time": "2024-08-28T13:27:10.474433Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9df6391c3d18e8faf9209fdecaa7406",
     "grade": false,
     "grade_id": "cell-3627cb068243e0db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Predefined Variables - Do Not Change their Name;\n",
    "This is a Read-Only cell. Remember to execute this cell once\"\"\"\n",
    "set_seeds(543) # Do not change this line\n",
    "input_layer = None  # Input layer\n",
    "op_1 = None  # Hidden layer 1\n",
    "op_2 = None  # Hidden layer 2\n",
    "output_layer = None  # Output layer\n",
    "nn_1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3773bf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:27:21.946285Z",
     "start_time": "2024-08-28T13:27:10.492825Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22791f97624f661c25a2211cbd317ea6",
     "grade": true,
     "grade_id": "cell-e370577f83ae1578",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 - 1s - loss: 1.0172 - accuracy: 0.9954 - val_loss: 0.8522 - val_accuracy: 0.9948 - 525ms/epoch - 525ms/step\n",
      "Epoch 2/1000\n",
      "1/1 - 0s - loss: 1.0083 - accuracy: 0.9954 - val_loss: 0.8447 - val_accuracy: 0.9947 - 21ms/epoch - 21ms/step\n",
      "Epoch 3/1000\n",
      "1/1 - 0s - loss: 0.9994 - accuracy: 0.9954 - val_loss: 0.8373 - val_accuracy: 0.9947 - 19ms/epoch - 19ms/step\n",
      "Epoch 4/1000\n",
      "1/1 - 0s - loss: 0.9906 - accuracy: 0.9954 - val_loss: 0.8298 - val_accuracy: 0.9947 - 19ms/epoch - 19ms/step\n",
      "Epoch 5/1000\n",
      "1/1 - 0s - loss: 0.9818 - accuracy: 0.9954 - val_loss: 0.8225 - val_accuracy: 0.9946 - 22ms/epoch - 22ms/step\n",
      "Epoch 6/1000\n",
      "1/1 - 0s - loss: 0.9731 - accuracy: 0.9954 - val_loss: 0.8151 - val_accuracy: 0.9946 - 30ms/epoch - 30ms/step\n",
      "Epoch 7/1000\n",
      "1/1 - 0s - loss: 0.9644 - accuracy: 0.9953 - val_loss: 0.8078 - val_accuracy: 0.9946 - 19ms/epoch - 19ms/step\n",
      "Epoch 8/1000\n",
      "1/1 - 0s - loss: 0.9557 - accuracy: 0.9953 - val_loss: 0.8005 - val_accuracy: 0.9946 - 19ms/epoch - 19ms/step\n",
      "Epoch 9/1000\n",
      "1/1 - 0s - loss: 0.9471 - accuracy: 0.9953 - val_loss: 0.7933 - val_accuracy: 0.9946 - 21ms/epoch - 21ms/step\n",
      "Epoch 10/1000\n",
      "1/1 - 0s - loss: 0.9385 - accuracy: 0.9953 - val_loss: 0.7861 - val_accuracy: 0.9946 - 34ms/epoch - 34ms/step\n",
      "Epoch 11/1000\n",
      "1/1 - 0s - loss: 0.9299 - accuracy: 0.9953 - val_loss: 0.7789 - val_accuracy: 0.9945 - 20ms/epoch - 20ms/step\n",
      "Epoch 12/1000\n",
      "1/1 - 0s - loss: 0.9214 - accuracy: 0.9952 - val_loss: 0.7718 - val_accuracy: 0.9945 - 20ms/epoch - 20ms/step\n",
      "Epoch 13/1000\n",
      "1/1 - 0s - loss: 0.9130 - accuracy: 0.9952 - val_loss: 0.7647 - val_accuracy: 0.9945 - 20ms/epoch - 20ms/step\n",
      "Epoch 14/1000\n",
      "1/1 - 0s - loss: 0.9046 - accuracy: 0.9952 - val_loss: 0.7577 - val_accuracy: 0.9945 - 36ms/epoch - 36ms/step\n",
      "Epoch 15/1000\n",
      "1/1 - 0s - loss: 0.8962 - accuracy: 0.9952 - val_loss: 0.7507 - val_accuracy: 0.9945 - 19ms/epoch - 19ms/step\n",
      "Epoch 16/1000\n",
      "1/1 - 0s - loss: 0.8879 - accuracy: 0.9951 - val_loss: 0.7437 - val_accuracy: 0.9945 - 20ms/epoch - 20ms/step\n",
      "Epoch 17/1000\n",
      "1/1 - 0s - loss: 0.8796 - accuracy: 0.9951 - val_loss: 0.7368 - val_accuracy: 0.9945 - 20ms/epoch - 20ms/step\n",
      "Epoch 18/1000\n",
      "1/1 - 0s - loss: 0.8714 - accuracy: 0.9951 - val_loss: 0.7299 - val_accuracy: 0.9945 - 33ms/epoch - 33ms/step\n",
      "Epoch 19/1000\n",
      "1/1 - 0s - loss: 0.8632 - accuracy: 0.9951 - val_loss: 0.7230 - val_accuracy: 0.9945 - 20ms/epoch - 20ms/step\n",
      "Epoch 20/1000\n",
      "1/1 - 0s - loss: 0.8551 - accuracy: 0.9951 - val_loss: 0.7162 - val_accuracy: 0.9944 - 19ms/epoch - 19ms/step\n",
      "Epoch 21/1000\n",
      "1/1 - 0s - loss: 0.8470 - accuracy: 0.9950 - val_loss: 0.7095 - val_accuracy: 0.9944 - 20ms/epoch - 20ms/step\n",
      "Epoch 22/1000\n",
      "1/1 - 0s - loss: 0.8390 - accuracy: 0.9950 - val_loss: 0.7027 - val_accuracy: 0.9944 - 41ms/epoch - 41ms/step\n",
      "Epoch 23/1000\n",
      "1/1 - 0s - loss: 0.8310 - accuracy: 0.9950 - val_loss: 0.6960 - val_accuracy: 0.9944 - 21ms/epoch - 21ms/step\n",
      "Epoch 24/1000\n",
      "1/1 - 0s - loss: 0.8231 - accuracy: 0.9949 - val_loss: 0.6894 - val_accuracy: 0.9944 - 20ms/epoch - 20ms/step\n",
      "Epoch 25/1000\n",
      "1/1 - 0s - loss: 0.8152 - accuracy: 0.9949 - val_loss: 0.6828 - val_accuracy: 0.9944 - 20ms/epoch - 20ms/step\n",
      "Epoch 26/1000\n",
      "1/1 - 0s - loss: 0.8073 - accuracy: 0.9949 - val_loss: 0.6762 - val_accuracy: 0.9944 - 41ms/epoch - 41ms/step\n",
      "Epoch 27/1000\n",
      "1/1 - 0s - loss: 0.7995 - accuracy: 0.9949 - val_loss: 0.6697 - val_accuracy: 0.9943 - 20ms/epoch - 20ms/step\n",
      "Epoch 28/1000\n",
      "1/1 - 0s - loss: 0.7917 - accuracy: 0.9949 - val_loss: 0.6632 - val_accuracy: 0.9942 - 19ms/epoch - 19ms/step\n",
      "Epoch 29/1000\n",
      "1/1 - 0s - loss: 0.7840 - accuracy: 0.9948 - val_loss: 0.6567 - val_accuracy: 0.9942 - 21ms/epoch - 21ms/step\n",
      "Epoch 30/1000\n",
      "1/1 - 0s - loss: 0.7764 - accuracy: 0.9948 - val_loss: 0.6503 - val_accuracy: 0.9941 - 35ms/epoch - 35ms/step\n",
      "Epoch 31/1000\n",
      "1/1 - 0s - loss: 0.7687 - accuracy: 0.9948 - val_loss: 0.6439 - val_accuracy: 0.9941 - 21ms/epoch - 21ms/step\n",
      "Epoch 32/1000\n",
      "1/1 - 0s - loss: 0.7612 - accuracy: 0.9948 - val_loss: 0.6376 - val_accuracy: 0.9941 - 19ms/epoch - 19ms/step\n",
      "Epoch 33/1000\n",
      "1/1 - 0s - loss: 0.7536 - accuracy: 0.9948 - val_loss: 0.6313 - val_accuracy: 0.9940 - 19ms/epoch - 19ms/step\n",
      "Epoch 34/1000\n",
      "1/1 - 0s - loss: 0.7461 - accuracy: 0.9948 - val_loss: 0.6250 - val_accuracy: 0.9940 - 35ms/epoch - 35ms/step\n",
      "Epoch 35/1000\n",
      "1/1 - 0s - loss: 0.7387 - accuracy: 0.9948 - val_loss: 0.6188 - val_accuracy: 0.9940 - 20ms/epoch - 20ms/step\n",
      "Epoch 36/1000\n",
      "1/1 - 0s - loss: 0.7313 - accuracy: 0.9948 - val_loss: 0.6126 - val_accuracy: 0.9939 - 21ms/epoch - 21ms/step\n",
      "Epoch 37/1000\n",
      "1/1 - 0s - loss: 0.7239 - accuracy: 0.9947 - val_loss: 0.6064 - val_accuracy: 0.9939 - 21ms/epoch - 21ms/step\n",
      "Epoch 38/1000\n",
      "1/1 - 0s - loss: 0.7166 - accuracy: 0.9947 - val_loss: 0.6003 - val_accuracy: 0.9939 - 35ms/epoch - 35ms/step\n",
      "Epoch 39/1000\n",
      "1/1 - 0s - loss: 0.7093 - accuracy: 0.9947 - val_loss: 0.5942 - val_accuracy: 0.9939 - 20ms/epoch - 20ms/step\n",
      "Epoch 40/1000\n",
      "1/1 - 0s - loss: 0.7021 - accuracy: 0.9947 - val_loss: 0.5882 - val_accuracy: 0.9939 - 20ms/epoch - 20ms/step\n",
      "Epoch 41/1000\n",
      "1/1 - 0s - loss: 0.6949 - accuracy: 0.9947 - val_loss: 0.5821 - val_accuracy: 0.9940 - 20ms/epoch - 20ms/step\n",
      "Epoch 42/1000\n",
      "1/1 - 0s - loss: 0.6878 - accuracy: 0.9948 - val_loss: 0.5762 - val_accuracy: 0.9940 - 33ms/epoch - 33ms/step\n",
      "Epoch 43/1000\n",
      "1/1 - 0s - loss: 0.6807 - accuracy: 0.9948 - val_loss: 0.5702 - val_accuracy: 0.9940 - 20ms/epoch - 20ms/step\n",
      "Epoch 44/1000\n",
      "1/1 - 0s - loss: 0.6736 - accuracy: 0.9948 - val_loss: 0.5643 - val_accuracy: 0.9940 - 20ms/epoch - 20ms/step\n",
      "Epoch 45/1000\n",
      "1/1 - 0s - loss: 0.6666 - accuracy: 0.9948 - val_loss: 0.5585 - val_accuracy: 0.9941 - 40ms/epoch - 40ms/step\n",
      "Epoch 46/1000\n",
      "1/1 - 0s - loss: 0.6596 - accuracy: 0.9948 - val_loss: 0.5526 - val_accuracy: 0.9941 - 20ms/epoch - 20ms/step\n",
      "Epoch 47/1000\n",
      "1/1 - 0s - loss: 0.6527 - accuracy: 0.9948 - val_loss: 0.5468 - val_accuracy: 0.9942 - 21ms/epoch - 21ms/step\n",
      "Epoch 48/1000\n",
      "1/1 - 0s - loss: 0.6458 - accuracy: 0.9948 - val_loss: 0.5410 - val_accuracy: 0.9942 - 21ms/epoch - 21ms/step\n",
      "Epoch 49/1000\n",
      "1/1 - 0s - loss: 0.6389 - accuracy: 0.9948 - val_loss: 0.5353 - val_accuracy: 0.9943 - 36ms/epoch - 36ms/step\n",
      "Epoch 50/1000\n",
      "1/1 - 0s - loss: 0.6321 - accuracy: 0.9949 - val_loss: 0.5296 - val_accuracy: 0.9944 - 19ms/epoch - 19ms/step\n",
      "Epoch 51/1000\n",
      "1/1 - 0s - loss: 0.6253 - accuracy: 0.9949 - val_loss: 0.5239 - val_accuracy: 0.9945 - 20ms/epoch - 20ms/step\n",
      "Epoch 52/1000\n",
      "1/1 - 0s - loss: 0.6186 - accuracy: 0.9951 - val_loss: 0.5183 - val_accuracy: 0.9946 - 21ms/epoch - 21ms/step\n",
      "Epoch 53/1000\n",
      "1/1 - 0s - loss: 0.6119 - accuracy: 0.9953 - val_loss: 0.5127 - val_accuracy: 0.9946 - 35ms/epoch - 35ms/step\n",
      "Epoch 54/1000\n",
      "1/1 - 0s - loss: 0.6052 - accuracy: 0.9953 - val_loss: 0.5071 - val_accuracy: 0.9946 - 20ms/epoch - 20ms/step\n",
      "Epoch 55/1000\n",
      "1/1 - 0s - loss: 0.5985 - accuracy: 0.9954 - val_loss: 0.5015 - val_accuracy: 0.9947 - 22ms/epoch - 22ms/step\n",
      "Epoch 56/1000\n",
      "1/1 - 0s - loss: 0.5919 - accuracy: 0.9954 - val_loss: 0.4960 - val_accuracy: 0.9948 - 22ms/epoch - 22ms/step\n",
      "Epoch 57/1000\n",
      "1/1 - 0s - loss: 0.5854 - accuracy: 0.9954 - val_loss: 0.4905 - val_accuracy: 0.9948 - 41ms/epoch - 41ms/step\n",
      "Epoch 58/1000\n",
      "1/1 - 0s - loss: 0.5788 - accuracy: 0.9954 - val_loss: 0.4851 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 59/1000\n",
      "1/1 - 0s - loss: 0.5723 - accuracy: 0.9954 - val_loss: 0.4796 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 60/1000\n",
      "1/1 - 0s - loss: 0.5658 - accuracy: 0.9954 - val_loss: 0.4742 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 61/1000\n",
      "1/1 - 0s - loss: 0.5594 - accuracy: 0.9954 - val_loss: 0.4688 - val_accuracy: 0.9948 - 39ms/epoch - 39ms/step\n",
      "Epoch 62/1000\n",
      "1/1 - 0s - loss: 0.5530 - accuracy: 0.9954 - val_loss: 0.4635 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 63/1000\n",
      "1/1 - 0s - loss: 0.5466 - accuracy: 0.9954 - val_loss: 0.4581 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 64/1000\n",
      "1/1 - 0s - loss: 0.5403 - accuracy: 0.9954 - val_loss: 0.4528 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 65/1000\n",
      "1/1 - 0s - loss: 0.5339 - accuracy: 0.9954 - val_loss: 0.4475 - val_accuracy: 0.9948 - 33ms/epoch - 33ms/step\n",
      "Epoch 66/1000\n",
      "1/1 - 0s - loss: 0.5277 - accuracy: 0.9954 - val_loss: 0.4423 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 67/1000\n",
      "1/1 - 0s - loss: 0.5214 - accuracy: 0.9954 - val_loss: 0.4370 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 68/1000\n",
      "1/1 - 0s - loss: 0.5152 - accuracy: 0.9954 - val_loss: 0.4318 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 69/1000\n",
      "1/1 - 0s - loss: 0.5089 - accuracy: 0.9954 - val_loss: 0.4266 - val_accuracy: 0.9948 - 38ms/epoch - 38ms/step\n",
      "Epoch 70/1000\n",
      "1/1 - 0s - loss: 0.5028 - accuracy: 0.9954 - val_loss: 0.4215 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 71/1000\n",
      "1/1 - 0s - loss: 0.4966 - accuracy: 0.9954 - val_loss: 0.4163 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 72/1000\n",
      "1/1 - 0s - loss: 0.4904 - accuracy: 0.9954 - val_loss: 0.4112 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 73/1000\n",
      "1/1 - 0s - loss: 0.4843 - accuracy: 0.9954 - val_loss: 0.4060 - val_accuracy: 0.9948 - 39ms/epoch - 39ms/step\n",
      "Epoch 74/1000\n",
      "1/1 - 0s - loss: 0.4782 - accuracy: 0.9954 - val_loss: 0.4009 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 75/1000\n",
      "1/1 - 0s - loss: 0.4722 - accuracy: 0.9954 - val_loss: 0.3958 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 76/1000\n",
      "1/1 - 0s - loss: 0.4661 - accuracy: 0.9954 - val_loss: 0.3908 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 77/1000\n",
      "1/1 - 0s - loss: 0.4600 - accuracy: 0.9954 - val_loss: 0.3857 - val_accuracy: 0.9948 - 36ms/epoch - 36ms/step\n",
      "Epoch 78/1000\n",
      "1/1 - 0s - loss: 0.4540 - accuracy: 0.9954 - val_loss: 0.3807 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 79/1000\n",
      "1/1 - 0s - loss: 0.4480 - accuracy: 0.9954 - val_loss: 0.3756 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 80/1000\n",
      "1/1 - 0s - loss: 0.4420 - accuracy: 0.9954 - val_loss: 0.3709 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 81/1000\n",
      "1/1 - 0s - loss: 0.4364 - accuracy: 0.9954 - val_loss: 0.3657 - val_accuracy: 0.9948 - 37ms/epoch - 37ms/step\n",
      "Epoch 82/1000\n",
      "1/1 - 0s - loss: 0.4302 - accuracy: 0.9954 - val_loss: 0.3607 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 83/1000\n",
      "1/1 - 0s - loss: 0.4243 - accuracy: 0.9954 - val_loss: 0.3558 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 84/1000\n",
      "1/1 - 0s - loss: 0.4184 - accuracy: 0.9954 - val_loss: 0.3509 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 85/1000\n",
      "1/1 - 0s - loss: 0.4125 - accuracy: 0.9954 - val_loss: 0.3460 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 86/1000\n",
      "1/1 - 0s - loss: 0.4067 - accuracy: 0.9954 - val_loss: 0.3411 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 87/1000\n",
      "1/1 - 0s - loss: 0.4009 - accuracy: 0.9954 - val_loss: 0.3363 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 88/1000\n",
      "1/1 - 0s - loss: 0.3950 - accuracy: 0.9954 - val_loss: 0.3314 - val_accuracy: 0.9948 - 35ms/epoch - 35ms/step\n",
      "Epoch 89/1000\n",
      "1/1 - 0s - loss: 0.3892 - accuracy: 0.9954 - val_loss: 0.3265 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 90/1000\n",
      "1/1 - 0s - loss: 0.3834 - accuracy: 0.9954 - val_loss: 0.3217 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 91/1000\n",
      "1/1 - 0s - loss: 0.3776 - accuracy: 0.9954 - val_loss: 0.3169 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 92/1000\n",
      "1/1 - 0s - loss: 0.3719 - accuracy: 0.9954 - val_loss: 0.3120 - val_accuracy: 0.9948 - 35ms/epoch - 35ms/step\n",
      "Epoch 93/1000\n",
      "1/1 - 0s - loss: 0.3661 - accuracy: 0.9954 - val_loss: 0.3072 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 94/1000\n",
      "1/1 - 0s - loss: 0.3604 - accuracy: 0.9954 - val_loss: 0.3024 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 95/1000\n",
      "1/1 - 0s - loss: 0.3546 - accuracy: 0.9954 - val_loss: 0.2976 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 96/1000\n",
      "1/1 - 0s - loss: 0.3489 - accuracy: 0.9954 - val_loss: 0.2928 - val_accuracy: 0.9948 - 36ms/epoch - 36ms/step\n",
      "Epoch 97/1000\n",
      "1/1 - 0s - loss: 0.3431 - accuracy: 0.9954 - val_loss: 0.2880 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 98/1000\n",
      "1/1 - 0s - loss: 0.3374 - accuracy: 0.9954 - val_loss: 0.2833 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 99/1000\n",
      "1/1 - 0s - loss: 0.3317 - accuracy: 0.9954 - val_loss: 0.2785 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 100/1000\n",
      "1/1 - 0s - loss: 0.3260 - accuracy: 0.9954 - val_loss: 0.2738 - val_accuracy: 0.9948 - 35ms/epoch - 35ms/step\n",
      "Epoch 101/1000\n",
      "1/1 - 0s - loss: 0.3204 - accuracy: 0.9954 - val_loss: 0.2691 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 102/1000\n",
      "1/1 - 0s - loss: 0.3147 - accuracy: 0.9954 - val_loss: 0.2644 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 103/1000\n",
      "1/1 - 0s - loss: 0.3091 - accuracy: 0.9954 - val_loss: 0.2597 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 104/1000\n",
      "1/1 - 0s - loss: 0.3034 - accuracy: 0.9954 - val_loss: 0.2550 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 105/1000\n",
      "1/1 - 0s - loss: 0.2977 - accuracy: 0.9954 - val_loss: 0.2503 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 106/1000\n",
      "1/1 - 0s - loss: 0.2921 - accuracy: 0.9954 - val_loss: 0.2456 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 107/1000\n",
      "1/1 - 0s - loss: 0.2865 - accuracy: 0.9954 - val_loss: 0.2409 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 108/1000\n",
      "1/1 - 0s - loss: 0.2809 - accuracy: 0.9954 - val_loss: 0.2363 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 109/1000\n",
      "1/1 - 0s - loss: 0.2753 - accuracy: 0.9954 - val_loss: 0.2316 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 110/1000\n",
      "1/1 - 0s - loss: 0.2697 - accuracy: 0.9954 - val_loss: 0.2270 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 111/1000\n",
      "1/1 - 0s - loss: 0.2641 - accuracy: 0.9954 - val_loss: 0.2224 - val_accuracy: 0.9948 - 22ms/epoch - 22ms/step\n",
      "Epoch 112/1000\n",
      "1/1 - 0s - loss: 0.2586 - accuracy: 0.9954 - val_loss: 0.2178 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 113/1000\n",
      "1/1 - 0s - loss: 0.2530 - accuracy: 0.9954 - val_loss: 0.2132 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 114/1000\n",
      "1/1 - 0s - loss: 0.2475 - accuracy: 0.9954 - val_loss: 0.2086 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 115/1000\n",
      "1/1 - 0s - loss: 0.2419 - accuracy: 0.9954 - val_loss: 0.2041 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 116/1000\n",
      "1/1 - 0s - loss: 0.2364 - accuracy: 0.9954 - val_loss: 0.1996 - val_accuracy: 0.9948 - 38ms/epoch - 38ms/step\n",
      "Epoch 117/1000\n",
      "1/1 - 0s - loss: 0.2309 - accuracy: 0.9954 - val_loss: 0.1950 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 118/1000\n",
      "1/1 - 0s - loss: 0.2255 - accuracy: 0.9954 - val_loss: 0.1905 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 119/1000\n",
      "1/1 - 0s - loss: 0.2200 - accuracy: 0.9954 - val_loss: 0.1861 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 120/1000\n",
      "1/1 - 0s - loss: 0.2146 - accuracy: 0.9954 - val_loss: 0.1816 - val_accuracy: 0.9948 - 34ms/epoch - 34ms/step\n",
      "Epoch 121/1000\n",
      "1/1 - 0s - loss: 0.2092 - accuracy: 0.9954 - val_loss: 0.1772 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 122/1000\n",
      "1/1 - 0s - loss: 0.2038 - accuracy: 0.9954 - val_loss: 0.1728 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 123/1000\n",
      "1/1 - 0s - loss: 0.1985 - accuracy: 0.9954 - val_loss: 0.1685 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 124/1000\n",
      "1/1 - 0s - loss: 0.1932 - accuracy: 0.9954 - val_loss: 0.1642 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 125/1000\n",
      "1/1 - 0s - loss: 0.1879 - accuracy: 0.9954 - val_loss: 0.1600 - val_accuracy: 0.9948 - 32ms/epoch - 32ms/step\n",
      "Epoch 126/1000\n",
      "1/1 - 0s - loss: 0.1827 - accuracy: 0.9954 - val_loss: 0.1558 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 127/1000\n",
      "1/1 - 0s - loss: 0.1775 - accuracy: 0.9954 - val_loss: 0.1517 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 128/1000\n",
      "1/1 - 0s - loss: 0.1724 - accuracy: 0.9954 - val_loss: 0.1476 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 129/1000\n",
      "1/1 - 0s - loss: 0.1673 - accuracy: 0.9954 - val_loss: 0.1437 - val_accuracy: 0.9948 - 33ms/epoch - 33ms/step\n",
      "Epoch 130/1000\n",
      "1/1 - 0s - loss: 0.1623 - accuracy: 0.9954 - val_loss: 0.1397 - val_accuracy: 0.9948 - 22ms/epoch - 22ms/step\n",
      "Epoch 131/1000\n",
      "1/1 - 0s - loss: 0.1575 - accuracy: 0.9954 - val_loss: 0.1359 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 132/1000\n",
      "1/1 - 0s - loss: 0.1527 - accuracy: 0.9954 - val_loss: 0.1323 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 133/1000\n",
      "1/1 - 0s - loss: 0.1480 - accuracy: 0.9954 - val_loss: 0.1287 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 134/1000\n",
      "1/1 - 0s - loss: 0.1434 - accuracy: 0.9954 - val_loss: 0.1253 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 135/1000\n",
      "1/1 - 0s - loss: 0.1390 - accuracy: 0.9954 - val_loss: 0.1220 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 136/1000\n",
      "1/1 - 0s - loss: 0.1347 - accuracy: 0.9954 - val_loss: 0.1189 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 137/1000\n",
      "1/1 - 0s - loss: 0.1306 - accuracy: 0.9954 - val_loss: 0.1160 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 138/1000\n",
      "1/1 - 0s - loss: 0.1267 - accuracy: 0.9954 - val_loss: 0.1133 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 139/1000\n",
      "1/1 - 0s - loss: 0.1230 - accuracy: 0.9954 - val_loss: 0.1109 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 140/1000\n",
      "1/1 - 0s - loss: 0.1196 - accuracy: 0.9954 - val_loss: 0.1088 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 141/1000\n",
      "1/1 - 0s - loss: 0.1165 - accuracy: 0.9954 - val_loss: 0.1069 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 142/1000\n",
      "1/1 - 0s - loss: 0.1137 - accuracy: 0.9954 - val_loss: 0.1055 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 143/1000\n",
      "1/1 - 0s - loss: 0.1114 - accuracy: 0.9954 - val_loss: 0.1044 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 144/1000\n",
      "1/1 - 0s - loss: 0.1091 - accuracy: 0.9954 - val_loss: 0.1039 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 145/1000\n",
      "1/1 - 0s - loss: 0.1078 - accuracy: 0.9954 - val_loss: 0.1035 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 146/1000\n",
      "1/1 - 0s - loss: 0.1065 - accuracy: 0.9954 - val_loss: 0.1032 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 147/1000\n",
      "1/1 - 0s - loss: 0.1054 - accuracy: 0.9954 - val_loss: 0.1030 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 148/1000\n",
      "1/1 - 0s - loss: 0.1048 - accuracy: 0.9954 - val_loss: 0.1028 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 149/1000\n",
      "1/1 - 0s - loss: 0.1043 - accuracy: 0.9954 - val_loss: 0.1025 - val_accuracy: 0.9948 - 34ms/epoch - 34ms/step\n",
      "Epoch 150/1000\n",
      "1/1 - 0s - loss: 0.1037 - accuracy: 0.9954 - val_loss: 0.1018 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 151/1000\n",
      "1/1 - 0s - loss: 0.1030 - accuracy: 0.9954 - val_loss: 0.1009 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 152/1000\n",
      "1/1 - 0s - loss: 0.1022 - accuracy: 0.9954 - val_loss: 0.0999 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 153/1000\n",
      "1/1 - 0s - loss: 0.1014 - accuracy: 0.9954 - val_loss: 0.0988 - val_accuracy: 0.9948 - 34ms/epoch - 34ms/step\n",
      "Epoch 154/1000\n",
      "1/1 - 0s - loss: 0.1006 - accuracy: 0.9954 - val_loss: 0.0978 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 155/1000\n",
      "1/1 - 0s - loss: 0.0999 - accuracy: 0.9954 - val_loss: 0.0969 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 156/1000\n",
      "1/1 - 0s - loss: 0.0993 - accuracy: 0.9954 - val_loss: 0.0961 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 157/1000\n",
      "1/1 - 0s - loss: 0.0987 - accuracy: 0.9954 - val_loss: 0.0954 - val_accuracy: 0.9948 - 35ms/epoch - 35ms/step\n",
      "Epoch 158/1000\n",
      "1/1 - 0s - loss: 0.0981 - accuracy: 0.9954 - val_loss: 0.0948 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 159/1000\n",
      "1/1 - 0s - loss: 0.0975 - accuracy: 0.9954 - val_loss: 0.0942 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 160/1000\n",
      "1/1 - 0s - loss: 0.0968 - accuracy: 0.9954 - val_loss: 0.0938 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 161/1000\n",
      "1/1 - 0s - loss: 0.0961 - accuracy: 0.9954 - val_loss: 0.0933 - val_accuracy: 0.9948 - 36ms/epoch - 36ms/step\n",
      "Epoch 162/1000\n",
      "1/1 - 0s - loss: 0.0954 - accuracy: 0.9954 - val_loss: 0.0930 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 163/1000\n",
      "1/1 - 0s - loss: 0.0948 - accuracy: 0.9954 - val_loss: 0.0926 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 164/1000\n",
      "1/1 - 0s - loss: 0.0942 - accuracy: 0.9954 - val_loss: 0.0923 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 165/1000\n",
      "1/1 - 0s - loss: 0.0937 - accuracy: 0.9954 - val_loss: 0.0919 - val_accuracy: 0.9948 - 36ms/epoch - 36ms/step\n",
      "Epoch 166/1000\n",
      "1/1 - 0s - loss: 0.0931 - accuracy: 0.9954 - val_loss: 0.0913 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 167/1000\n",
      "1/1 - 0s - loss: 0.0925 - accuracy: 0.9954 - val_loss: 0.0907 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 168/1000\n",
      "1/1 - 0s - loss: 0.0919 - accuracy: 0.9954 - val_loss: 0.0900 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 169/1000\n",
      "1/1 - 0s - loss: 0.0913 - accuracy: 0.9954 - val_loss: 0.0893 - val_accuracy: 0.9948 - 34ms/epoch - 34ms/step\n",
      "Epoch 170/1000\n",
      "1/1 - 0s - loss: 0.0908 - accuracy: 0.9954 - val_loss: 0.0886 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 171/1000\n",
      "1/1 - 0s - loss: 0.0902 - accuracy: 0.9954 - val_loss: 0.0880 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 172/1000\n",
      "1/1 - 0s - loss: 0.0897 - accuracy: 0.9954 - val_loss: 0.0874 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 173/1000\n",
      "1/1 - 0s - loss: 0.0892 - accuracy: 0.9954 - val_loss: 0.0869 - val_accuracy: 0.9948 - 36ms/epoch - 36ms/step\n",
      "Epoch 174/1000\n",
      "1/1 - 0s - loss: 0.0886 - accuracy: 0.9954 - val_loss: 0.0864 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 175/1000\n",
      "1/1 - 0s - loss: 0.0881 - accuracy: 0.9954 - val_loss: 0.0860 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 176/1000\n",
      "1/1 - 0s - loss: 0.0876 - accuracy: 0.9954 - val_loss: 0.0856 - val_accuracy: 0.9948 - 24ms/epoch - 24ms/step\n",
      "Epoch 177/1000\n",
      "1/1 - 0s - loss: 0.0871 - accuracy: 0.9954 - val_loss: 0.0853 - val_accuracy: 0.9948 - 35ms/epoch - 35ms/step\n",
      "Epoch 178/1000\n",
      "1/1 - 0s - loss: 0.0865 - accuracy: 0.9954 - val_loss: 0.0849 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 179/1000\n",
      "1/1 - 0s - loss: 0.0861 - accuracy: 0.9954 - val_loss: 0.0845 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 180/1000\n",
      "1/1 - 0s - loss: 0.0856 - accuracy: 0.9954 - val_loss: 0.0841 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 181/1000\n",
      "1/1 - 0s - loss: 0.0851 - accuracy: 0.9954 - val_loss: 0.0836 - val_accuracy: 0.9948 - 34ms/epoch - 34ms/step\n",
      "Epoch 182/1000\n",
      "1/1 - 0s - loss: 0.0846 - accuracy: 0.9954 - val_loss: 0.0831 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 183/1000\n",
      "1/1 - 0s - loss: 0.0841 - accuracy: 0.9954 - val_loss: 0.0826 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 184/1000\n",
      "1/1 - 0s - loss: 0.0836 - accuracy: 0.9954 - val_loss: 0.0820 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 185/1000\n",
      "1/1 - 0s - loss: 0.0832 - accuracy: 0.9954 - val_loss: 0.0815 - val_accuracy: 0.9948 - 37ms/epoch - 37ms/step\n",
      "Epoch 186/1000\n",
      "1/1 - 0s - loss: 0.0827 - accuracy: 0.9954 - val_loss: 0.0811 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 187/1000\n",
      "1/1 - 0s - loss: 0.0823 - accuracy: 0.9954 - val_loss: 0.0806 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 188/1000\n",
      "1/1 - 0s - loss: 0.0818 - accuracy: 0.9954 - val_loss: 0.0803 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 189/1000\n",
      "1/1 - 0s - loss: 0.0814 - accuracy: 0.9954 - val_loss: 0.0799 - val_accuracy: 0.9948 - 40ms/epoch - 40ms/step\n",
      "Epoch 190/1000\n",
      "1/1 - 0s - loss: 0.0809 - accuracy: 0.9954 - val_loss: 0.0795 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 191/1000\n",
      "1/1 - 0s - loss: 0.0805 - accuracy: 0.9954 - val_loss: 0.0792 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 192/1000\n",
      "1/1 - 0s - loss: 0.0800 - accuracy: 0.9954 - val_loss: 0.0788 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 193/1000\n",
      "1/1 - 0s - loss: 0.0796 - accuracy: 0.9954 - val_loss: 0.0785 - val_accuracy: 0.9948 - 37ms/epoch - 37ms/step\n",
      "Epoch 194/1000\n",
      "1/1 - 0s - loss: 0.0792 - accuracy: 0.9954 - val_loss: 0.0780 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 195/1000\n",
      "1/1 - 0s - loss: 0.0788 - accuracy: 0.9954 - val_loss: 0.0776 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 196/1000\n",
      "1/1 - 0s - loss: 0.0784 - accuracy: 0.9954 - val_loss: 0.0772 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 197/1000\n",
      "1/1 - 0s - loss: 0.0779 - accuracy: 0.9954 - val_loss: 0.0767 - val_accuracy: 0.9948 - 33ms/epoch - 33ms/step\n",
      "Epoch 198/1000\n",
      "1/1 - 0s - loss: 0.0775 - accuracy: 0.9954 - val_loss: 0.0763 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 199/1000\n",
      "1/1 - 0s - loss: 0.0771 - accuracy: 0.9954 - val_loss: 0.0759 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 200/1000\n",
      "1/1 - 0s - loss: 0.0767 - accuracy: 0.9954 - val_loss: 0.0756 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 201/1000\n",
      "1/1 - 0s - loss: 0.0763 - accuracy: 0.9954 - val_loss: 0.0752 - val_accuracy: 0.9948 - 35ms/epoch - 35ms/step\n",
      "Epoch 202/1000\n",
      "1/1 - 0s - loss: 0.0759 - accuracy: 0.9954 - val_loss: 0.0749 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 203/1000\n",
      "1/1 - 0s - loss: 0.0755 - accuracy: 0.9954 - val_loss: 0.0746 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 204/1000\n",
      "1/1 - 0s - loss: 0.0752 - accuracy: 0.9954 - val_loss: 0.0743 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 205/1000\n",
      "1/1 - 0s - loss: 0.0748 - accuracy: 0.9954 - val_loss: 0.0739 - val_accuracy: 0.9948 - 34ms/epoch - 34ms/step\n",
      "Epoch 206/1000\n",
      "1/1 - 0s - loss: 0.0744 - accuracy: 0.9954 - val_loss: 0.0735 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 207/1000\n",
      "1/1 - 0s - loss: 0.0740 - accuracy: 0.9954 - val_loss: 0.0732 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 208/1000\n",
      "1/1 - 0s - loss: 0.0736 - accuracy: 0.9954 - val_loss: 0.0728 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 209/1000\n",
      "1/1 - 0s - loss: 0.0733 - accuracy: 0.9954 - val_loss: 0.0724 - val_accuracy: 0.9948 - 32ms/epoch - 32ms/step\n",
      "Epoch 210/1000\n",
      "1/1 - 0s - loss: 0.0729 - accuracy: 0.9954 - val_loss: 0.0721 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 211/1000\n",
      "1/1 - 0s - loss: 0.0725 - accuracy: 0.9954 - val_loss: 0.0717 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 212/1000\n",
      "1/1 - 0s - loss: 0.0722 - accuracy: 0.9954 - val_loss: 0.0714 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 213/1000\n",
      "1/1 - 0s - loss: 0.0718 - accuracy: 0.9954 - val_loss: 0.0711 - val_accuracy: 0.9948 - 35ms/epoch - 35ms/step\n",
      "Epoch 214/1000\n",
      "1/1 - 0s - loss: 0.0715 - accuracy: 0.9954 - val_loss: 0.0708 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 215/1000\n",
      "1/1 - 0s - loss: 0.0711 - accuracy: 0.9954 - val_loss: 0.0705 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 216/1000\n",
      "1/1 - 0s - loss: 0.0708 - accuracy: 0.9954 - val_loss: 0.0702 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 217/1000\n",
      "1/1 - 0s - loss: 0.0704 - accuracy: 0.9954 - val_loss: 0.0698 - val_accuracy: 0.9948 - 32ms/epoch - 32ms/step\n",
      "Epoch 218/1000\n",
      "1/1 - 0s - loss: 0.0701 - accuracy: 0.9954 - val_loss: 0.0695 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 219/1000\n",
      "1/1 - 0s - loss: 0.0697 - accuracy: 0.9954 - val_loss: 0.0691 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 220/1000\n",
      "1/1 - 0s - loss: 0.0694 - accuracy: 0.9954 - val_loss: 0.0688 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 221/1000\n",
      "1/1 - 0s - loss: 0.0691 - accuracy: 0.9954 - val_loss: 0.0685 - val_accuracy: 0.9948 - 35ms/epoch - 35ms/step\n",
      "Epoch 222/1000\n",
      "1/1 - 0s - loss: 0.0687 - accuracy: 0.9954 - val_loss: 0.0682 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 223/1000\n",
      "1/1 - 0s - loss: 0.0684 - accuracy: 0.9954 - val_loss: 0.0679 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 224/1000\n",
      "1/1 - 0s - loss: 0.0681 - accuracy: 0.9954 - val_loss: 0.0676 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 225/1000\n",
      "1/1 - 0s - loss: 0.0677 - accuracy: 0.9954 - val_loss: 0.0673 - val_accuracy: 0.9948 - 36ms/epoch - 36ms/step\n",
      "Epoch 226/1000\n",
      "1/1 - 0s - loss: 0.0674 - accuracy: 0.9954 - val_loss: 0.0670 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 227/1000\n",
      "1/1 - 0s - loss: 0.0671 - accuracy: 0.9954 - val_loss: 0.0667 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 228/1000\n",
      "1/1 - 0s - loss: 0.0668 - accuracy: 0.9954 - val_loss: 0.0664 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 229/1000\n",
      "1/1 - 0s - loss: 0.0665 - accuracy: 0.9954 - val_loss: 0.0661 - val_accuracy: 0.9948 - 36ms/epoch - 36ms/step\n",
      "Epoch 230/1000\n",
      "1/1 - 0s - loss: 0.0661 - accuracy: 0.9954 - val_loss: 0.0658 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 231/1000\n",
      "1/1 - 0s - loss: 0.0658 - accuracy: 0.9954 - val_loss: 0.0655 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 232/1000\n",
      "1/1 - 0s - loss: 0.0655 - accuracy: 0.9954 - val_loss: 0.0653 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 233/1000\n",
      "1/1 - 0s - loss: 0.0652 - accuracy: 0.9954 - val_loss: 0.0650 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 234/1000\n",
      "1/1 - 0s - loss: 0.0649 - accuracy: 0.9954 - val_loss: 0.0647 - val_accuracy: 0.9948 - 33ms/epoch - 33ms/step\n",
      "Epoch 235/1000\n",
      "1/1 - 0s - loss: 0.0646 - accuracy: 0.9954 - val_loss: 0.0644 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 236/1000\n",
      "1/1 - 0s - loss: 0.0643 - accuracy: 0.9954 - val_loss: 0.0642 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 237/1000\n",
      "1/1 - 0s - loss: 0.0640 - accuracy: 0.9954 - val_loss: 0.0639 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 238/1000\n",
      "1/1 - 0s - loss: 0.0637 - accuracy: 0.9954 - val_loss: 0.0636 - val_accuracy: 0.9948 - 36ms/epoch - 36ms/step\n",
      "Epoch 239/1000\n",
      "1/1 - 0s - loss: 0.0634 - accuracy: 0.9954 - val_loss: 0.0633 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 240/1000\n",
      "1/1 - 0s - loss: 0.0631 - accuracy: 0.9954 - val_loss: 0.0631 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 241/1000\n",
      "1/1 - 0s - loss: 0.0628 - accuracy: 0.9954 - val_loss: 0.0628 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 242/1000\n",
      "1/1 - 0s - loss: 0.0626 - accuracy: 0.9954 - val_loss: 0.0625 - val_accuracy: 0.9948 - 37ms/epoch - 37ms/step\n",
      "Epoch 243/1000\n",
      "1/1 - 0s - loss: 0.0623 - accuracy: 0.9954 - val_loss: 0.0623 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 244/1000\n",
      "1/1 - 0s - loss: 0.0620 - accuracy: 0.9954 - val_loss: 0.0620 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 245/1000\n",
      "1/1 - 0s - loss: 0.0617 - accuracy: 0.9954 - val_loss: 0.0618 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 246/1000\n",
      "1/1 - 0s - loss: 0.0614 - accuracy: 0.9954 - val_loss: 0.0615 - val_accuracy: 0.9948 - 34ms/epoch - 34ms/step\n",
      "Epoch 247/1000\n",
      "1/1 - 0s - loss: 0.0612 - accuracy: 0.9954 - val_loss: 0.0612 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 248/1000\n",
      "1/1 - 0s - loss: 0.0609 - accuracy: 0.9954 - val_loss: 0.0610 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 249/1000\n",
      "1/1 - 0s - loss: 0.0606 - accuracy: 0.9954 - val_loss: 0.0607 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 250/1000\n",
      "1/1 - 0s - loss: 0.0603 - accuracy: 0.9954 - val_loss: 0.0605 - val_accuracy: 0.9948 - 35ms/epoch - 35ms/step\n",
      "Epoch 251/1000\n",
      "1/1 - 0s - loss: 0.0601 - accuracy: 0.9954 - val_loss: 0.0602 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 252/1000\n",
      "1/1 - 0s - loss: 0.0598 - accuracy: 0.9954 - val_loss: 0.0600 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 253/1000\n",
      "1/1 - 0s - loss: 0.0595 - accuracy: 0.9954 - val_loss: 0.0598 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 254/1000\n",
      "1/1 - 0s - loss: 0.0593 - accuracy: 0.9954 - val_loss: 0.0595 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 255/1000\n",
      "1/1 - 0s - loss: 0.0590 - accuracy: 0.9954 - val_loss: 0.0593 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 256/1000\n",
      "1/1 - 0s - loss: 0.0588 - accuracy: 0.9954 - val_loss: 0.0590 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 257/1000\n",
      "1/1 - 0s - loss: 0.0585 - accuracy: 0.9954 - val_loss: 0.0588 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 258/1000\n",
      "1/1 - 0s - loss: 0.0582 - accuracy: 0.9954 - val_loss: 0.0586 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 259/1000\n",
      "1/1 - 0s - loss: 0.0580 - accuracy: 0.9954 - val_loss: 0.0583 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 260/1000\n",
      "1/1 - 0s - loss: 0.0577 - accuracy: 0.9954 - val_loss: 0.0581 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 261/1000\n",
      "1/1 - 0s - loss: 0.0575 - accuracy: 0.9954 - val_loss: 0.0579 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 262/1000\n",
      "1/1 - 0s - loss: 0.0572 - accuracy: 0.9954 - val_loss: 0.0576 - val_accuracy: 0.9948 - 33ms/epoch - 33ms/step\n",
      "Epoch 263/1000\n",
      "1/1 - 0s - loss: 0.0570 - accuracy: 0.9954 - val_loss: 0.0574 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 264/1000\n",
      "1/1 - 0s - loss: 0.0567 - accuracy: 0.9954 - val_loss: 0.0572 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 265/1000\n",
      "1/1 - 0s - loss: 0.0565 - accuracy: 0.9954 - val_loss: 0.0570 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 266/1000\n",
      "1/1 - 0s - loss: 0.0563 - accuracy: 0.9954 - val_loss: 0.0567 - val_accuracy: 0.9948 - 36ms/epoch - 36ms/step\n",
      "Epoch 267/1000\n",
      "1/1 - 0s - loss: 0.0560 - accuracy: 0.9954 - val_loss: 0.0565 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 268/1000\n",
      "1/1 - 0s - loss: 0.0558 - accuracy: 0.9954 - val_loss: 0.0563 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 269/1000\n",
      "1/1 - 0s - loss: 0.0555 - accuracy: 0.9954 - val_loss: 0.0561 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 270/1000\n",
      "1/1 - 0s - loss: 0.0553 - accuracy: 0.9954 - val_loss: 0.0559 - val_accuracy: 0.9948 - 36ms/epoch - 36ms/step\n",
      "Epoch 271/1000\n",
      "1/1 - 0s - loss: 0.0551 - accuracy: 0.9954 - val_loss: 0.0556 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 272/1000\n",
      "1/1 - 0s - loss: 0.0548 - accuracy: 0.9954 - val_loss: 0.0554 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 273/1000\n",
      "1/1 - 0s - loss: 0.0546 - accuracy: 0.9954 - val_loss: 0.0552 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 274/1000\n",
      "1/1 - 0s - loss: 0.0544 - accuracy: 0.9954 - val_loss: 0.0550 - val_accuracy: 0.9948 - 35ms/epoch - 35ms/step\n",
      "Epoch 275/1000\n",
      "1/1 - 0s - loss: 0.0542 - accuracy: 0.9954 - val_loss: 0.0548 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 276/1000\n",
      "1/1 - 0s - loss: 0.0539 - accuracy: 0.9954 - val_loss: 0.0546 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 277/1000\n",
      "1/1 - 0s - loss: 0.0537 - accuracy: 0.9954 - val_loss: 0.0544 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 278/1000\n",
      "1/1 - 0s - loss: 0.0535 - accuracy: 0.9954 - val_loss: 0.0542 - val_accuracy: 0.9948 - 34ms/epoch - 34ms/step\n",
      "Epoch 279/1000\n",
      "1/1 - 0s - loss: 0.0533 - accuracy: 0.9954 - val_loss: 0.0540 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 280/1000\n",
      "1/1 - 0s - loss: 0.0530 - accuracy: 0.9954 - val_loss: 0.0538 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 281/1000\n",
      "1/1 - 0s - loss: 0.0528 - accuracy: 0.9954 - val_loss: 0.0536 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 282/1000\n",
      "1/1 - 0s - loss: 0.0526 - accuracy: 0.9954 - val_loss: 0.0534 - val_accuracy: 0.9948 - 36ms/epoch - 36ms/step\n",
      "Epoch 283/1000\n",
      "1/1 - 0s - loss: 0.0524 - accuracy: 0.9954 - val_loss: 0.0532 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 284/1000\n",
      "1/1 - 0s - loss: 0.0522 - accuracy: 0.9954 - val_loss: 0.0530 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 285/1000\n",
      "1/1 - 0s - loss: 0.0520 - accuracy: 0.9954 - val_loss: 0.0528 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 286/1000\n",
      "1/1 - 0s - loss: 0.0518 - accuracy: 0.9954 - val_loss: 0.0526 - val_accuracy: 0.9948 - 35ms/epoch - 35ms/step\n",
      "Epoch 287/1000\n",
      "1/1 - 0s - loss: 0.0516 - accuracy: 0.9954 - val_loss: 0.0524 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 288/1000\n",
      "1/1 - 0s - loss: 0.0514 - accuracy: 0.9954 - val_loss: 0.0522 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 289/1000\n",
      "1/1 - 0s - loss: 0.0511 - accuracy: 0.9954 - val_loss: 0.0520 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 290/1000\n",
      "1/1 - 0s - loss: 0.0509 - accuracy: 0.9954 - val_loss: 0.0518 - val_accuracy: 0.9948 - 36ms/epoch - 36ms/step\n",
      "Epoch 291/1000\n",
      "1/1 - 0s - loss: 0.0507 - accuracy: 0.9954 - val_loss: 0.0517 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 292/1000\n",
      "1/1 - 0s - loss: 0.0505 - accuracy: 0.9954 - val_loss: 0.0515 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 293/1000\n",
      "1/1 - 0s - loss: 0.0503 - accuracy: 0.9954 - val_loss: 0.0513 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 294/1000\n",
      "1/1 - 0s - loss: 0.0501 - accuracy: 0.9954 - val_loss: 0.0511 - val_accuracy: 0.9948 - 36ms/epoch - 36ms/step\n",
      "Epoch 295/1000\n",
      "1/1 - 0s - loss: 0.0499 - accuracy: 0.9954 - val_loss: 0.0509 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 296/1000\n",
      "1/1 - 0s - loss: 0.0497 - accuracy: 0.9954 - val_loss: 0.0508 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 297/1000\n",
      "1/1 - 0s - loss: 0.0496 - accuracy: 0.9954 - val_loss: 0.0506 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 298/1000\n",
      "1/1 - 0s - loss: 0.0494 - accuracy: 0.9954 - val_loss: 0.0504 - val_accuracy: 0.9948 - 33ms/epoch - 33ms/step\n",
      "Epoch 299/1000\n",
      "1/1 - 0s - loss: 0.0492 - accuracy: 0.9954 - val_loss: 0.0502 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 300/1000\n",
      "1/1 - 0s - loss: 0.0490 - accuracy: 0.9954 - val_loss: 0.0501 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 301/1000\n",
      "1/1 - 0s - loss: 0.0488 - accuracy: 0.9954 - val_loss: 0.0499 - val_accuracy: 0.9948 - 22ms/epoch - 22ms/step\n",
      "Epoch 302/1000\n",
      "1/1 - 0s - loss: 0.0486 - accuracy: 0.9954 - val_loss: 0.0497 - val_accuracy: 0.9948 - 32ms/epoch - 32ms/step\n",
      "Epoch 303/1000\n",
      "1/1 - 0s - loss: 0.0484 - accuracy: 0.9954 - val_loss: 0.0496 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 304/1000\n",
      "1/1 - 0s - loss: 0.0482 - accuracy: 0.9954 - val_loss: 0.0494 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 305/1000\n",
      "1/1 - 0s - loss: 0.0480 - accuracy: 0.9954 - val_loss: 0.0492 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 306/1000\n",
      "1/1 - 0s - loss: 0.0479 - accuracy: 0.9954 - val_loss: 0.0490 - val_accuracy: 0.9948 - 32ms/epoch - 32ms/step\n",
      "Epoch 307/1000\n",
      "1/1 - 0s - loss: 0.0477 - accuracy: 0.9954 - val_loss: 0.0489 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 308/1000\n",
      "1/1 - 0s - loss: 0.0475 - accuracy: 0.9954 - val_loss: 0.0487 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 309/1000\n",
      "1/1 - 0s - loss: 0.0473 - accuracy: 0.9954 - val_loss: 0.0486 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 310/1000\n",
      "1/1 - 0s - loss: 0.0471 - accuracy: 0.9954 - val_loss: 0.0484 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 311/1000\n",
      "1/1 - 0s - loss: 0.0470 - accuracy: 0.9954 - val_loss: 0.0482 - val_accuracy: 0.9948 - 33ms/epoch - 33ms/step\n",
      "Epoch 312/1000\n",
      "1/1 - 0s - loss: 0.0468 - accuracy: 0.9954 - val_loss: 0.0481 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 313/1000\n",
      "1/1 - 0s - loss: 0.0466 - accuracy: 0.9954 - val_loss: 0.0479 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 314/1000\n",
      "1/1 - 0s - loss: 0.0464 - accuracy: 0.9954 - val_loss: 0.0478 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 315/1000\n",
      "1/1 - 0s - loss: 0.0463 - accuracy: 0.9954 - val_loss: 0.0476 - val_accuracy: 0.9948 - 37ms/epoch - 37ms/step\n",
      "Epoch 316/1000\n",
      "1/1 - 0s - loss: 0.0461 - accuracy: 0.9954 - val_loss: 0.0475 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 317/1000\n",
      "1/1 - 0s - loss: 0.0459 - accuracy: 0.9954 - val_loss: 0.0473 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 318/1000\n",
      "1/1 - 0s - loss: 0.0458 - accuracy: 0.9954 - val_loss: 0.0471 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 319/1000\n",
      "1/1 - 0s - loss: 0.0456 - accuracy: 0.9954 - val_loss: 0.0470 - val_accuracy: 0.9948 - 39ms/epoch - 39ms/step\n",
      "Epoch 320/1000\n",
      "1/1 - 0s - loss: 0.0454 - accuracy: 0.9954 - val_loss: 0.0469 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 321/1000\n",
      "1/1 - 0s - loss: 0.0453 - accuracy: 0.9954 - val_loss: 0.0467 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 322/1000\n",
      "1/1 - 0s - loss: 0.0451 - accuracy: 0.9954 - val_loss: 0.0465 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 323/1000\n",
      "1/1 - 0s - loss: 0.0450 - accuracy: 0.9954 - val_loss: 0.0464 - val_accuracy: 0.9948 - 34ms/epoch - 34ms/step\n",
      "Epoch 324/1000\n",
      "1/1 - 0s - loss: 0.0448 - accuracy: 0.9954 - val_loss: 0.0463 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 325/1000\n",
      "1/1 - 0s - loss: 0.0446 - accuracy: 0.9954 - val_loss: 0.0461 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 326/1000\n",
      "1/1 - 0s - loss: 0.0445 - accuracy: 0.9954 - val_loss: 0.0460 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 327/1000\n",
      "1/1 - 0s - loss: 0.0443 - accuracy: 0.9954 - val_loss: 0.0458 - val_accuracy: 0.9948 - 33ms/epoch - 33ms/step\n",
      "Epoch 328/1000\n",
      "1/1 - 0s - loss: 0.0442 - accuracy: 0.9954 - val_loss: 0.0457 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 329/1000\n",
      "1/1 - 0s - loss: 0.0440 - accuracy: 0.9954 - val_loss: 0.0456 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 330/1000\n",
      "1/1 - 0s - loss: 0.0439 - accuracy: 0.9954 - val_loss: 0.0454 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 331/1000\n",
      "1/1 - 0s - loss: 0.0437 - accuracy: 0.9954 - val_loss: 0.0453 - val_accuracy: 0.9948 - 32ms/epoch - 32ms/step\n",
      "Epoch 332/1000\n",
      "1/1 - 0s - loss: 0.0435 - accuracy: 0.9954 - val_loss: 0.0451 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 333/1000\n",
      "1/1 - 0s - loss: 0.0434 - accuracy: 0.9954 - val_loss: 0.0450 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 334/1000\n",
      "1/1 - 0s - loss: 0.0432 - accuracy: 0.9954 - val_loss: 0.0449 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 335/1000\n",
      "1/1 - 0s - loss: 0.0431 - accuracy: 0.9954 - val_loss: 0.0447 - val_accuracy: 0.9948 - 29ms/epoch - 29ms/step\n",
      "Epoch 336/1000\n",
      "1/1 - 0s - loss: 0.0430 - accuracy: 0.9954 - val_loss: 0.0446 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 337/1000\n",
      "1/1 - 0s - loss: 0.0428 - accuracy: 0.9954 - val_loss: 0.0445 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 338/1000\n",
      "1/1 - 0s - loss: 0.0427 - accuracy: 0.9954 - val_loss: 0.0443 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 339/1000\n",
      "1/1 - 0s - loss: 0.0425 - accuracy: 0.9954 - val_loss: 0.0442 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 340/1000\n",
      "1/1 - 0s - loss: 0.0424 - accuracy: 0.9954 - val_loss: 0.0441 - val_accuracy: 0.9948 - 35ms/epoch - 35ms/step\n",
      "Epoch 341/1000\n",
      "1/1 - 0s - loss: 0.0422 - accuracy: 0.9954 - val_loss: 0.0439 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 342/1000\n",
      "1/1 - 0s - loss: 0.0421 - accuracy: 0.9954 - val_loss: 0.0438 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 343/1000\n",
      "1/1 - 0s - loss: 0.0420 - accuracy: 0.9954 - val_loss: 0.0437 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 344/1000\n",
      "1/1 - 0s - loss: 0.0418 - accuracy: 0.9954 - val_loss: 0.0435 - val_accuracy: 0.9948 - 34ms/epoch - 34ms/step\n",
      "Epoch 345/1000\n",
      "1/1 - 0s - loss: 0.0417 - accuracy: 0.9954 - val_loss: 0.0434 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 346/1000\n",
      "1/1 - 0s - loss: 0.0415 - accuracy: 0.9954 - val_loss: 0.0434 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 347/1000\n",
      "1/1 - 0s - loss: 0.0414 - accuracy: 0.9954 - val_loss: 0.0432 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 348/1000\n",
      "1/1 - 0s - loss: 0.0413 - accuracy: 0.9954 - val_loss: 0.0430 - val_accuracy: 0.9948 - 28ms/epoch - 28ms/step\n",
      "Epoch 349/1000\n",
      "1/1 - 0s - loss: 0.0412 - accuracy: 0.9954 - val_loss: 0.0430 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 350/1000\n",
      "1/1 - 0s - loss: 0.0410 - accuracy: 0.9954 - val_loss: 0.0428 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 351/1000\n",
      "1/1 - 0s - loss: 0.0409 - accuracy: 0.9954 - val_loss: 0.0427 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 352/1000\n",
      "1/1 - 0s - loss: 0.0408 - accuracy: 0.9954 - val_loss: 0.0426 - val_accuracy: 0.9948 - 36ms/epoch - 36ms/step\n",
      "Epoch 353/1000\n",
      "1/1 - 0s - loss: 0.0406 - accuracy: 0.9954 - val_loss: 0.0426 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 354/1000\n",
      "1/1 - 0s - loss: 0.0405 - accuracy: 0.9954 - val_loss: 0.0424 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 355/1000\n",
      "1/1 - 0s - loss: 0.0404 - accuracy: 0.9954 - val_loss: 0.0422 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 356/1000\n",
      "1/1 - 0s - loss: 0.0403 - accuracy: 0.9954 - val_loss: 0.0421 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 357/1000\n",
      "1/1 - 0s - loss: 0.0401 - accuracy: 0.9954 - val_loss: 0.0421 - val_accuracy: 0.9948 - 32ms/epoch - 32ms/step\n",
      "Epoch 358/1000\n",
      "1/1 - 0s - loss: 0.0400 - accuracy: 0.9954 - val_loss: 0.0420 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 359/1000\n",
      "1/1 - 0s - loss: 0.0399 - accuracy: 0.9954 - val_loss: 0.0418 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 360/1000\n",
      "1/1 - 0s - loss: 0.0398 - accuracy: 0.9954 - val_loss: 0.0417 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 361/1000\n",
      "1/1 - 0s - loss: 0.0397 - accuracy: 0.9954 - val_loss: 0.0416 - val_accuracy: 0.9948 - 38ms/epoch - 38ms/step\n",
      "Epoch 362/1000\n",
      "1/1 - 0s - loss: 0.0395 - accuracy: 0.9954 - val_loss: 0.0416 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 363/1000\n",
      "1/1 - 0s - loss: 0.0394 - accuracy: 0.9954 - val_loss: 0.0414 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 364/1000\n",
      "1/1 - 0s - loss: 0.0393 - accuracy: 0.9954 - val_loss: 0.0413 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 365/1000\n",
      "1/1 - 0s - loss: 0.0392 - accuracy: 0.9954 - val_loss: 0.0412 - val_accuracy: 0.9948 - 33ms/epoch - 33ms/step\n",
      "Epoch 366/1000\n",
      "1/1 - 0s - loss: 0.0391 - accuracy: 0.9954 - val_loss: 0.0411 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 367/1000\n",
      "1/1 - 0s - loss: 0.0389 - accuracy: 0.9954 - val_loss: 0.0410 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 368/1000\n",
      "1/1 - 0s - loss: 0.0388 - accuracy: 0.9954 - val_loss: 0.0409 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 369/1000\n",
      "1/1 - 0s - loss: 0.0387 - accuracy: 0.9954 - val_loss: 0.0408 - val_accuracy: 0.9948 - 31ms/epoch - 31ms/step\n",
      "Epoch 370/1000\n",
      "1/1 - 0s - loss: 0.0386 - accuracy: 0.9954 - val_loss: 0.0407 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 371/1000\n",
      "1/1 - 0s - loss: 0.0385 - accuracy: 0.9954 - val_loss: 0.0406 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 372/1000\n",
      "1/1 - 0s - loss: 0.0384 - accuracy: 0.9954 - val_loss: 0.0405 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 373/1000\n",
      "1/1 - 0s - loss: 0.0383 - accuracy: 0.9954 - val_loss: 0.0404 - val_accuracy: 0.9948 - 30ms/epoch - 30ms/step\n",
      "Epoch 374/1000\n",
      "1/1 - 0s - loss: 0.0382 - accuracy: 0.9954 - val_loss: 0.0403 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 375/1000\n",
      "1/1 - 0s - loss: 0.0380 - accuracy: 0.9954 - val_loss: 0.0402 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 376/1000\n",
      "1/1 - 0s - loss: 0.0379 - accuracy: 0.9954 - val_loss: 0.0401 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 377/1000\n",
      "1/1 - 0s - loss: 0.0378 - accuracy: 0.9954 - val_loss: 0.0400 - val_accuracy: 0.9948 - 35ms/epoch - 35ms/step\n",
      "Epoch 378/1000\n",
      "1/1 - 0s - loss: 0.0377 - accuracy: 0.9954 - val_loss: 0.0399 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 379/1000\n",
      "1/1 - 0s - loss: 0.0376 - accuracy: 0.9954 - val_loss: 0.0398 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 380/1000\n",
      "1/1 - 0s - loss: 0.0375 - accuracy: 0.9954 - val_loss: 0.0397 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 381/1000\n",
      "1/1 - 0s - loss: 0.0374 - accuracy: 0.9954 - val_loss: 0.0396 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 382/1000\n",
      "1/1 - 0s - loss: 0.0373 - accuracy: 0.9954 - val_loss: 0.0395 - val_accuracy: 0.9948 - 31ms/epoch - 31ms/step\n",
      "Epoch 383/1000\n",
      "1/1 - 0s - loss: 0.0372 - accuracy: 0.9954 - val_loss: 0.0394 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 384/1000\n",
      "1/1 - 0s - loss: 0.0371 - accuracy: 0.9954 - val_loss: 0.0394 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 385/1000\n",
      "1/1 - 0s - loss: 0.0370 - accuracy: 0.9954 - val_loss: 0.0393 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 386/1000\n",
      "1/1 - 0s - loss: 0.0369 - accuracy: 0.9954 - val_loss: 0.0392 - val_accuracy: 0.9948 - 38ms/epoch - 38ms/step\n",
      "Epoch 387/1000\n",
      "1/1 - 0s - loss: 0.0368 - accuracy: 0.9954 - val_loss: 0.0391 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 388/1000\n",
      "1/1 - 0s - loss: 0.0367 - accuracy: 0.9954 - val_loss: 0.0390 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 389/1000\n",
      "1/1 - 0s - loss: 0.0366 - accuracy: 0.9954 - val_loss: 0.0389 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 390/1000\n",
      "1/1 - 0s - loss: 0.0365 - accuracy: 0.9954 - val_loss: 0.0388 - val_accuracy: 0.9948 - 34ms/epoch - 34ms/step\n",
      "Epoch 391/1000\n",
      "1/1 - 0s - loss: 0.0364 - accuracy: 0.9954 - val_loss: 0.0387 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 392/1000\n",
      "1/1 - 0s - loss: 0.0363 - accuracy: 0.9954 - val_loss: 0.0387 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 393/1000\n",
      "1/1 - 0s - loss: 0.0362 - accuracy: 0.9954 - val_loss: 0.0386 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 394/1000\n",
      "1/1 - 0s - loss: 0.0361 - accuracy: 0.9954 - val_loss: 0.0385 - val_accuracy: 0.9948 - 31ms/epoch - 31ms/step\n",
      "Epoch 395/1000\n",
      "1/1 - 0s - loss: 0.0360 - accuracy: 0.9954 - val_loss: 0.0384 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 396/1000\n",
      "1/1 - 0s - loss: 0.0359 - accuracy: 0.9954 - val_loss: 0.0383 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 397/1000\n",
      "1/1 - 0s - loss: 0.0358 - accuracy: 0.9954 - val_loss: 0.0382 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 398/1000\n",
      "1/1 - 0s - loss: 0.0357 - accuracy: 0.9954 - val_loss: 0.0381 - val_accuracy: 0.9948 - 35ms/epoch - 35ms/step\n",
      "Epoch 399/1000\n",
      "1/1 - 0s - loss: 0.0357 - accuracy: 0.9954 - val_loss: 0.0381 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 400/1000\n",
      "1/1 - 0s - loss: 0.0356 - accuracy: 0.9954 - val_loss: 0.0380 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 401/1000\n",
      "1/1 - 0s - loss: 0.0355 - accuracy: 0.9954 - val_loss: 0.0379 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 402/1000\n",
      "1/1 - 0s - loss: 0.0354 - accuracy: 0.9954 - val_loss: 0.0378 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 403/1000\n",
      "1/1 - 0s - loss: 0.0353 - accuracy: 0.9954 - val_loss: 0.0377 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 404/1000\n",
      "1/1 - 0s - loss: 0.0352 - accuracy: 0.9954 - val_loss: 0.0377 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 405/1000\n",
      "1/1 - 0s - loss: 0.0351 - accuracy: 0.9954 - val_loss: 0.0376 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 406/1000\n",
      "1/1 - 0s - loss: 0.0350 - accuracy: 0.9954 - val_loss: 0.0375 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 407/1000\n",
      "1/1 - 0s - loss: 0.0350 - accuracy: 0.9954 - val_loss: 0.0374 - val_accuracy: 0.9948 - 33ms/epoch - 33ms/step\n",
      "Epoch 408/1000\n",
      "1/1 - 0s - loss: 0.0349 - accuracy: 0.9954 - val_loss: 0.0374 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 409/1000\n",
      "1/1 - 0s - loss: 0.0348 - accuracy: 0.9954 - val_loss: 0.0373 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 410/1000\n",
      "1/1 - 0s - loss: 0.0347 - accuracy: 0.9954 - val_loss: 0.0372 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 411/1000\n",
      "1/1 - 0s - loss: 0.0346 - accuracy: 0.9954 - val_loss: 0.0371 - val_accuracy: 0.9948 - 33ms/epoch - 33ms/step\n",
      "Epoch 412/1000\n",
      "1/1 - 0s - loss: 0.0345 - accuracy: 0.9954 - val_loss: 0.0371 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 413/1000\n",
      "1/1 - 0s - loss: 0.0345 - accuracy: 0.9954 - val_loss: 0.0370 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 414/1000\n",
      "1/1 - 0s - loss: 0.0344 - accuracy: 0.9954 - val_loss: 0.0369 - val_accuracy: 0.9948 - 18ms/epoch - 18ms/step\n",
      "Epoch 415/1000\n",
      "1/1 - 0s - loss: 0.0343 - accuracy: 0.9954 - val_loss: 0.0369 - val_accuracy: 0.9948 - 40ms/epoch - 40ms/step\n",
      "Epoch 416/1000\n",
      "1/1 - 0s - loss: 0.0342 - accuracy: 0.9954 - val_loss: 0.0368 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 417/1000\n",
      "1/1 - 0s - loss: 0.0341 - accuracy: 0.9954 - val_loss: 0.0367 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 418/1000\n",
      "1/1 - 0s - loss: 0.0340 - accuracy: 0.9954 - val_loss: 0.0366 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 419/1000\n",
      "1/1 - 0s - loss: 0.0340 - accuracy: 0.9954 - val_loss: 0.0366 - val_accuracy: 0.9948 - 31ms/epoch - 31ms/step\n",
      "Epoch 420/1000\n",
      "1/1 - 0s - loss: 0.0339 - accuracy: 0.9954 - val_loss: 0.0365 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 421/1000\n",
      "1/1 - 0s - loss: 0.0338 - accuracy: 0.9954 - val_loss: 0.0364 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 422/1000\n",
      "1/1 - 0s - loss: 0.0337 - accuracy: 0.9954 - val_loss: 0.0364 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 423/1000\n",
      "1/1 - 0s - loss: 0.0337 - accuracy: 0.9954 - val_loss: 0.0363 - val_accuracy: 0.9948 - 36ms/epoch - 36ms/step\n",
      "Epoch 424/1000\n",
      "1/1 - 0s - loss: 0.0336 - accuracy: 0.9954 - val_loss: 0.0362 - val_accuracy: 0.9948 - 22ms/epoch - 22ms/step\n",
      "Epoch 425/1000\n",
      "1/1 - 0s - loss: 0.0335 - accuracy: 0.9954 - val_loss: 0.0362 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 426/1000\n",
      "1/1 - 0s - loss: 0.0334 - accuracy: 0.9954 - val_loss: 0.0361 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 427/1000\n",
      "1/1 - 0s - loss: 0.0334 - accuracy: 0.9954 - val_loss: 0.0361 - val_accuracy: 0.9948 - 28ms/epoch - 28ms/step\n",
      "Epoch 428/1000\n",
      "1/1 - 0s - loss: 0.0333 - accuracy: 0.9954 - val_loss: 0.0360 - val_accuracy: 0.9948 - 22ms/epoch - 22ms/step\n",
      "Epoch 429/1000\n",
      "1/1 - 0s - loss: 0.0332 - accuracy: 0.9954 - val_loss: 0.0359 - val_accuracy: 0.9948 - 19ms/epoch - 19ms/step\n",
      "Epoch 430/1000\n",
      "1/1 - 0s - loss: 0.0332 - accuracy: 0.9954 - val_loss: 0.0359 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 431/1000\n",
      "1/1 - 0s - loss: 0.0331 - accuracy: 0.9954 - val_loss: 0.0358 - val_accuracy: 0.9948 - 31ms/epoch - 31ms/step\n",
      "Epoch 432/1000\n",
      "1/1 - 0s - loss: 0.0330 - accuracy: 0.9954 - val_loss: 0.0357 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 433/1000\n",
      "1/1 - 0s - loss: 0.0330 - accuracy: 0.9954 - val_loss: 0.0357 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n",
      "Epoch 434/1000\n",
      "1/1 - 0s - loss: 0.0329 - accuracy: 0.9954 - val_loss: 0.0357 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 435/1000\n",
      "1/1 - 0s - loss: 0.0328 - accuracy: 0.9954 - val_loss: 0.0356 - val_accuracy: 0.9948 - 34ms/epoch - 34ms/step\n",
      "Epoch 436/1000\n",
      "1/1 - 0s - loss: 0.0329 - accuracy: 0.9954 - val_loss: 0.0355 - val_accuracy: 0.9948 - 23ms/epoch - 23ms/step\n",
      "Epoch 437/1000\n",
      "1/1 - 0s - loss: 0.0327 - accuracy: 0.9954 - val_loss: 0.0357 - val_accuracy: 0.9948 - 20ms/epoch - 20ms/step\n",
      "Epoch 438/1000\n",
      "1/1 - 0s - loss: 0.0330 - accuracy: 0.9954 - val_loss: 0.0356 - val_accuracy: 0.9948 - 21ms/epoch - 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f638c63f7f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Populate the variables shown above with appropriate values here\"\"\"\n",
    "# BEGIN - YOUR CODE GOES HERE\n",
    "drop_features = ['TX_DATETIME', 'CUSTOMER_ID', 'TERMINAL_ID','TX_FRAUD']\n",
    "X_train = df_train.drop(columns=drop_features)  \n",
    "y_train = df_train[\"TX_FRAUD\"]  \n",
    "X_test = df_test.drop(columns=drop_features)  \n",
    "y_test = df_test[\"TX_FRAUD\"]  \n",
    "\n",
    "clear_session()\n",
    "\n",
    "units = 8\n",
    "output_units, output_activation = (1, 'sigmoid')\n",
    "\n",
    "hidden_layer_activation = \"ReLU\"\n",
    "input_shape = X_train.shape[1:]\n",
    "input_layer = Input(shape=input_shape, name=\"input_layer\")\n",
    "\n",
    "op_1 = Dense(units=units, activation=hidden_layer_activation, name=\"hidden_layer_1\")(input_layer)\n",
    "op_2 = Dense(units=units, activation=hidden_layer_activation, name=\"hidden_layer_2\")(op_1)\n",
    "output_layer = Dense(units=output_units, activation=output_activation, name=\"output_layer\")(op_2)\n",
    "\n",
    "nn_1 = Model(inputs=[input_layer], outputs=[output_layer])\n",
    "nn_1.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set training parameters\n",
    "epochs = 1000\n",
    "patience = 2\n",
    "verbose = 2\n",
    "batch_size = X_train.shape[0]\n",
    "\n",
    "# Define the early stopping callback\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=patience)]\n",
    "\n",
    "# Train and validate the model\n",
    "nn_1.fit(X_train, y_train,\n",
    "                   epochs=epochs,\n",
    "                   callbacks=callbacks,\n",
    "                   validation_data=(X_test, y_test),\n",
    "                   batch_size=batch_size,\n",
    "                   verbose=verbose)\n",
    "\n",
    "\n",
    "# END - YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1938296",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:27:21.956247Z",
     "start_time": "2024-08-28T13:27:21.947540Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5cb8db401af4a79b2536d6ca953e7c3f",
     "grade": false,
     "grade_id": "cell-b3e30bb48ebc057c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 1)]               0         \n",
      "                                                                 \n",
      " hidden_layer_1 (Dense)      (None, 8)                 16        \n",
      "                                                                 \n",
      " hidden_layer_2 (Dense)      (None, 8)                 72        \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 97 (388.00 Byte)\n",
      "Trainable params: 97 (388.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This is a Read-Only cell. Remember to execute this cell once\"\"\"\n",
    "try:\n",
    "    nn_1.summary()\n",
    "except NameError:\n",
    "    print(f\"Did you forget to run the readonly cell above?\")\n",
    "except AttributeError:\n",
    "    print(f\"Your code is possibly incorrect for creating nn_1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221bee02",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "925df21c5993e6511ee8f4a2c27a58b5",
     "grade": false,
     "grade_id": "cell-1e3893a1ce97377b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong> Q2 (iii) [<font color=\"red\"> 0.5 marks </font>]</strong> Calculate the AUC metric for the trained logistic regression model and the trained neural network model using the test dataset. You should use sklearn metrics to compute AUC scores and use the given variables to store the AUC metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fc71e8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:27:21.971540Z",
     "start_time": "2024-08-28T13:27:21.957280Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "315e28cf274f059421019729d42d207a",
     "grade": false,
     "grade_id": "cell-8ff3986954c1a78b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Predefined Variables - Do Not Change their Name;\n",
    "This is a Read-Only cell. Remember to execute this cell once\"\"\"\n",
    "logit_1_auc = None\n",
    "nn_1_auc = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e6fa1a4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T14:06:45.025264Z",
     "start_time": "2024-08-28T14:06:44.560729Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21fe285dfcb29ce62ab00254924c099b",
     "grade": true,
     "grade_id": "cell-2c0febaa88607768",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486/486 [==============================] - 0s 557us/step\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Populate the variables shown above with appropriate values here\"\"\"\n",
    "# BEGIN - YOUR CODE GOES HERE\n",
    "# Get predicted probabilities for both models\n",
    "X_test = sm.add_constant(X_test)\n",
    "logit_1_probs = logit_1.predict(X_test) \n",
    "logit_1_auc = round(roc_auc_score(y_test, logit_1_probs),3)\n",
    "X_test.drop(columns = \"const\", axis = 1, inplace = True)\n",
    "\n",
    "nn_1_probs = nn_1.predict(X_test).ravel()  \n",
    "nn_1_auc = round(roc_auc_score(y_test, nn_1_probs),3)\n",
    "\n",
    "# END - YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f82a2f89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T14:06:46.670316Z",
     "start_time": "2024-08-28T14:06:46.667068Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d71b76c6124a8ed61951ac6af22cd6e",
     "grade": false,
     "grade_id": "cell-311baf9e34fd24f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_1_auc is 0.634\n",
      "nn_1_auc is 0.366\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This is a Read-Only cell. Remember to execute this cell once\"\"\"\n",
    "try:\n",
    "    print(f\"logit_1_auc is {logit_1_auc}\")\n",
    "    print(f\"nn_1_auc is {nn_1_auc}\")\n",
    "except NameError:\n",
    "    print(f\"Did you forget to run the readonly cell above?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0299af93",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d6a7f074a49864f708d65909618c0e3",
     "grade": false,
     "grade_id": "cell-bec068fc18865b86",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong> Q2 (iv) [<font color=\"red\"> 0.5 marks </font>]</strong> Write the AUC scores for the logit and neural network models. Are the obtained AUC scores good? Is it advisable to train a large neural network model (e.g., 100+ layers) on the given data set? Justify your answer.  [Word limit < 150 words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26a53d9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f722f993d05d9036159f97c2aa2e687",
     "grade": false,
     "grade_id": "cell-6b5634243cd1f0ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<span class=\"label label-info text-uppercase\">Note</span>\n",
    "<span>Write your justification in the Markdown cell below</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e2425",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0db5f26216c302255b84559f61b39ce",
     "grade": true,
     "grade_id": "cell-98365dd4e33956d7",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The AUC scores for the models are 0.634 for the logistic regression model and 0.366 for the neural network model. These scores indicate that the logistic regression model performs better than the neural network model in distinguishing between fraudulent and non-fraudulent transactions. An AUC score of 0.634 is not high but still shows some predictive power, while a score of 0.366 is quite low and suggests that the neural network model is performing worse than random guessing.\n",
    "Given these results, it is not advisable to train a large neural network model with 100+ layers on this dataset. The poor performance of the current neural network suggests that the model is struggling with the data, possibly due to insufficient feature representation or overfitting. A larger model could exacerbate these issues, leading to even worse performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75bbfad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a93497a881336d61c714de1794f0624",
     "grade": false,
     "grade_id": "cell-f28a85c628a1214e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 3 [5.5 marks]\n",
    "\n",
    "<strong> Q3 (i) [<font color=\"red\"> 0.5 marks </font>]</strong> Add three columns in the `df_transactions` dataframe:\n",
    "1. A column named `TX_DATE` should have the date of the transaction without any time information.\n",
    "2. A column named `TX_WEEKDAY` should have the day of the week (Monday is 0 and Sunday is 6).\n",
    "3. A column name `TX_WEEK` should have the week number in which the transaction occurred (e.g., first week of January is week 1, the second week of January is week 2 and so forth). A week is defined as starting on a Monday and ending on a Sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0cbb060",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:27:23.428955Z",
     "start_time": "2024-08-28T13:27:22.503655Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bb42ce861d9fa30e75f3e377cf10f3a",
     "grade": true,
     "grade_id": "cell-e6535426007ab657",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TX_DATETIME</th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>TERMINAL_ID</th>\n",
       "      <th>TX_AMOUNT</th>\n",
       "      <th>TX_FRAUD</th>\n",
       "      <th>TX_DATE</th>\n",
       "      <th>TX_WEEKDAY</th>\n",
       "      <th>TX_WEEK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRANSACTION_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>730399</th>\n",
       "      <td>2024-03-02 16:58:04</td>\n",
       "      <td>1610</td>\n",
       "      <td>7393</td>\n",
       "      <td>122.82</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-02</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718607</th>\n",
       "      <td>2024-03-02 03:54:17</td>\n",
       "      <td>1610</td>\n",
       "      <td>3193</td>\n",
       "      <td>96.39</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-02</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732845</th>\n",
       "      <td>2024-03-02 22:20:50</td>\n",
       "      <td>1610</td>\n",
       "      <td>2239</td>\n",
       "      <td>167.10</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-02</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726608</th>\n",
       "      <td>2024-03-02 12:59:55</td>\n",
       "      <td>1610</td>\n",
       "      <td>9572</td>\n",
       "      <td>121.51</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-02</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729587</th>\n",
       "      <td>2024-03-02 15:54:29</td>\n",
       "      <td>1610</td>\n",
       "      <td>9850</td>\n",
       "      <td>191.29</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-02</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TX_DATETIME  CUSTOMER_ID  TERMINAL_ID  TX_AMOUNT  \\\n",
       "TRANSACTION_ID                                                            \n",
       "730399         2024-03-02 16:58:04         1610         7393     122.82   \n",
       "718607         2024-03-02 03:54:17         1610         3193      96.39   \n",
       "732845         2024-03-02 22:20:50         1610         2239     167.10   \n",
       "726608         2024-03-02 12:59:55         1610         9572     121.51   \n",
       "729587         2024-03-02 15:54:29         1610         9850     191.29   \n",
       "\n",
       "                TX_FRAUD     TX_DATE  TX_WEEKDAY  TX_WEEK  \n",
       "TRANSACTION_ID                                             \n",
       "730399                 0  2024-03-02           5        9  \n",
       "718607                 0  2024-03-02           5        9  \n",
       "732845                 0  2024-03-02           5        9  \n",
       "726608                 0  2024-03-02           5        9  \n",
       "729587                 0  2024-03-02           5        9  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Add the columns discussed above with appropriate values here\"\"\"\n",
    "# BEGIN - YOUR CODE GOES HERE\n",
    "df_transactions[\"TX_DATE\"] = df_transactions[\"TX_DATETIME\"].dt.date\n",
    "df_transactions[\"TX_WEEKDAY\"] = df_transactions[\"TX_DATETIME\"].apply(lambda dt: dt.weekday())\n",
    "df_transactions[\"TX_WEEK\"] = df_transactions[\"TX_DATETIME\"].apply(lambda date: date.isocalendar().week)\n",
    "df_transactions.head()\n",
    "# END - YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51096583",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "566e417ae3f04c33cf52e46c5a1e298a",
     "grade": false,
     "grade_id": "cell-58ea8b2284c4f298",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong> Q3 (ii) [<font color=\"red\"> 0.5 marks </font>]</strong> Generate a line plot to show the number of fraudulent transactions that occur in relation to each week day. The x-axis should have the number of the week day (Monday is 0 and Sunday is 6), and the y-axis should show the number of transactions on that week day. Label your plot appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bc053f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T11:33:45.138807Z",
     "start_time": "2024-08-31T11:33:45.077589Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e26ff45762a65e54916cb47d47d10b6",
     "grade": true,
     "grade_id": "cell-7345be3e62acd9c6",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TX_WEEKDAY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/home/common/jh/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/common/jh/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/common/jh/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TX_WEEKDAY'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2090502/2325518089.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# BEGIN - YOUR CODE GOES HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfrauds_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_transactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TX_FRAUD\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_transactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TX_WEEKDAY\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrauds_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrauds_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of fraudulent transactions by weekday'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/common/jh/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3453\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3454\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3455\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/common/jh/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TX_WEEKDAY'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Generate your plot here\"\"\"\n",
    "# BEGIN - YOUR CODE GOES HERE\n",
    "plt.figure(figsize = (10,6))\n",
    "frauds_count = df_transactions[\"TX_FRAUD\"].groupby(df_transactions[\"TX_WEEKDAY\"]).count()\n",
    "plt.plot(frauds_count.index, frauds_count.values, marker='o')\n",
    "plt.title('Number of fraudulent transactions by weekday')\n",
    "plt.xlabel('Weekday')\n",
    "plt.ylabel('Number of fraudulent transactions')\n",
    "plt.xticks(ticks=range(7), labels=[calendar.day_name[i] for i in range(7)])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# END - YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f18db1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8f0ab937b9e22c2347e12bf101e9c37",
     "grade": false,
     "grade_id": "cell-3bfe9a7a6d0a4f79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong> Q3 (iii) [<font color=\"red\"> 0.5 marks </font>]</strong> Eve claims that the number of fraudulent transactions are highest on weekends. Hence, adding a feature to denote if a transaction occurred on a weekend or not is an important feature to have in credit card fraud detection. Is Eve's claim true?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10df2ed7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:27:23.562508Z",
     "start_time": "2024-08-28T13:27:23.527363Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53c968b2bc6a7bd9908dea877d29deb6",
     "grade": true,
     "grade_id": "cell-88037ff9530910ad",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekends fraudulent transactions percent: 49.89%\n",
      "Other days fraudulent transactions percent: 50.11%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Write your code here for your justification below\"\"\"\n",
    "# BEGIN - YOUR CODE GOES HERE\n",
    "is_weekend = df_transactions['TX_WEEKDAY'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "weekend_frauds = df_transactions[\"TX_FRAUD\"].groupby(is_weekend).count()\n",
    "weekend_frauds_percent = round((weekend_frauds[1]/weekend_frauds.sum())*100, 2)\n",
    "weekday_frauds_percent = round((weekend_frauds[0]/weekend_frauds.sum())*100, 2)\n",
    "\n",
    "print(f\"Weekends fraudulent transactions percent: {weekend_frauds_percent}%\" + \"\\n\" +\n",
    "      f\"Other days fraudulent transactions percent: {weekday_frauds_percent}%\")\n",
    "# END - YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50adcaf8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "684f0a293ce8eecfe702749e1e731501",
     "grade": false,
     "grade_id": "cell-96084f326068a3f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<span class=\"label label-info text-uppercase\">Note</span>\n",
    "<span>Write your justification in the Markdown cell below</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81e9a43",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "898c93a6f6570db050b9a7fe22d66c08",
     "grade": true,
     "grade_id": "cell-588b865ff80acea1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "#### WRITE YOUR ANSWER(S) HERE IN THIS CELL\n",
    "Based on the results provided, the difference is relatively small (49.89% on weekends vs. 50.11% on other days), indicating that The number of fraudulent transactions is very close between weekends and other days. However, as the weekends occupy 2 days out of 7 days of the week, we only expected around 28.57% of fraudulent transactions during this period. This suggests a disproportionate number of fraudulent transactions occurring on weekends compared to what would be expected, and therefore we conclude that Eve's statement is true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad698414",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7969b8775290058e560eebcca0577c1",
     "grade": false,
     "grade_id": "cell-0781e0290be53283",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong> Q3 (iv) [<font color=\"red\"> 1.5 marks </font>]</strong> This question asks you to create column that will measure the *Risk* of a terminal over a day. In particular, we are interested in the ratio of fraudulent transactions arising from a terminal on the previous day. The column that should be added to the `df_transactions` dataframe is:\n",
    "1. `TERMINAL_RISK_1D:`: The ratio of fraudulent transactions on the previous day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e24797d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:27:23.660737Z",
     "start_time": "2024-08-28T13:27:23.563582Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0df850e528dd205e9d65379e326a338",
     "grade": true,
     "grade_id": "cell-141ac8987d129741",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TX_DATETIME</th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>TERMINAL_ID</th>\n",
       "      <th>TX_AMOUNT</th>\n",
       "      <th>TX_FRAUD</th>\n",
       "      <th>TX_DATE</th>\n",
       "      <th>TX_WEEKDAY</th>\n",
       "      <th>TX_WEEK</th>\n",
       "      <th>TERMINAL_RISK_1D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3988</th>\n",
       "      <td>2024-03-01 00:00:35</td>\n",
       "      <td>1866</td>\n",
       "      <td>5557</td>\n",
       "      <td>90.53</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25572</th>\n",
       "      <td>2024-03-01 00:00:58</td>\n",
       "      <td>286</td>\n",
       "      <td>2776</td>\n",
       "      <td>84.85</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12314</th>\n",
       "      <td>2024-03-01 00:01:59</td>\n",
       "      <td>4475</td>\n",
       "      <td>4716</td>\n",
       "      <td>125.81</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26980</th>\n",
       "      <td>2024-03-01 00:02:22</td>\n",
       "      <td>950</td>\n",
       "      <td>6093</td>\n",
       "      <td>155.13</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9594</th>\n",
       "      <td>2024-03-01 00:04:48</td>\n",
       "      <td>2384</td>\n",
       "      <td>4039</td>\n",
       "      <td>148.01</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              TX_DATETIME  CUSTOMER_ID  TERMINAL_ID  TX_AMOUNT  TX_FRAUD  \\\n",
       "3988  2024-03-01 00:00:35         1866         5557      90.53         0   \n",
       "25572 2024-03-01 00:00:58          286         2776      84.85         0   \n",
       "12314 2024-03-01 00:01:59         4475         4716     125.81         0   \n",
       "26980 2024-03-01 00:02:22          950         6093     155.13         0   \n",
       "9594  2024-03-01 00:04:48         2384         4039     148.01         0   \n",
       "\n",
       "          TX_DATE  TX_WEEKDAY  TX_WEEK  TERMINAL_RISK_1D  \n",
       "3988   2024-03-01           4        9               NaN  \n",
       "25572  2024-03-01           4        9               NaN  \n",
       "12314  2024-03-01           4        9               NaN  \n",
       "26980  2024-03-01           4        9               NaN  \n",
       "9594   2024-03-01           4        9               NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Add the columns discussed above with appropriate values here\"\"\"\n",
    "# BEGIN - YOUR CODE GOES HERE\n",
    "grouped_cols = [\"TERMINAL_ID\", \"TX_DATE\"]\n",
    "terminal_stats = df_transactions.groupby(grouped_cols).agg(\n",
    "    total_transactions=(\"TX_FRAUD\", \"count\"),\n",
    "    fraudulent_transactions=(\"TX_FRAUD\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "terminal_stats['daily_risk'] = terminal_stats['fraudulent_transactions'] / terminal_stats['total_transactions']\n",
    "terminal_stats['TERMINAL_RISK_1D'] = terminal_stats.groupby(['TERMINAL_ID'])['daily_risk'].shift()\n",
    "df_transactions = pd.merge(df_transactions, terminal_stats[['TERMINAL_ID', 'TX_DATE', 'TERMINAL_RISK_1D']],\n",
    "                           on=['TERMINAL_ID', 'TX_DATE'], \n",
    "                           how='left')\n",
    "df_transactions.sort_values(by= \"TX_DATETIME\").head()\n",
    "# END - YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed094cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94d7eff70864f8594884a8fa74ecd637",
     "grade": false,
     "grade_id": "cell-572bd1c1cc9bd36b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong> Q3 (v) [<font color=\"red\"> 1.5 marks </font>]</strong> This question asks you to create columns that will store the expected *Monetary* value of transactions for customers. In particular, we are interested in the *median* value of transactions a customer did on the previous day and in the previous week (where week is defined as starting on a Monday and ending on a Sunday). The columns should be added to the `df_transactions` dataframe as per:\n",
    "1. `SPENT_1D`: The median dollar value of transactions for this customer on previous day\n",
    "2.  `SPENT_1W`: The median dollar value of transactions for this customer on previous week\n",
    "\n",
    "*Note* The `df_transactions` dataframe should not have any columns that are not required as per this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf0d43b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:27:23.750381Z",
     "start_time": "2024-08-28T13:27:23.661947Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9697e9545902878e7d8230738f3c1ca",
     "grade": true,
     "grade_id": "cell-dc1d7a75a3fb2331",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TX_DATETIME</th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>TERMINAL_ID</th>\n",
       "      <th>TX_AMOUNT</th>\n",
       "      <th>TX_FRAUD</th>\n",
       "      <th>TX_DATE</th>\n",
       "      <th>TX_WEEKDAY</th>\n",
       "      <th>TX_WEEK</th>\n",
       "      <th>TERMINAL_RISK_1D</th>\n",
       "      <th>SPENT_1D</th>\n",
       "      <th>SPENT_1W</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3988</th>\n",
       "      <td>2024-03-01 00:00:35</td>\n",
       "      <td>1866</td>\n",
       "      <td>5557</td>\n",
       "      <td>90.53</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25572</th>\n",
       "      <td>2024-03-01 00:00:58</td>\n",
       "      <td>286</td>\n",
       "      <td>2776</td>\n",
       "      <td>84.85</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12314</th>\n",
       "      <td>2024-03-01 00:01:59</td>\n",
       "      <td>4475</td>\n",
       "      <td>4716</td>\n",
       "      <td>125.81</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26980</th>\n",
       "      <td>2024-03-01 00:02:22</td>\n",
       "      <td>950</td>\n",
       "      <td>6093</td>\n",
       "      <td>155.13</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9594</th>\n",
       "      <td>2024-03-01 00:04:48</td>\n",
       "      <td>2384</td>\n",
       "      <td>4039</td>\n",
       "      <td>148.01</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              TX_DATETIME  CUSTOMER_ID  TERMINAL_ID  TX_AMOUNT  TX_FRAUD  \\\n",
       "3988  2024-03-01 00:00:35         1866         5557      90.53         0   \n",
       "25572 2024-03-01 00:00:58          286         2776      84.85         0   \n",
       "12314 2024-03-01 00:01:59         4475         4716     125.81         0   \n",
       "26980 2024-03-01 00:02:22          950         6093     155.13         0   \n",
       "9594  2024-03-01 00:04:48         2384         4039     148.01         0   \n",
       "\n",
       "         TX_DATE  TX_WEEKDAY  TX_WEEK  TERMINAL_RISK_1D  SPENT_1D  SPENT_1W  \n",
       "3988  2024-03-01           4        9               NaN       NaN       NaN  \n",
       "25572 2024-03-01           4        9               NaN       NaN       NaN  \n",
       "12314 2024-03-01           4        9               NaN       NaN       NaN  \n",
       "26980 2024-03-01           4        9               NaN       NaN       NaN  \n",
       "9594  2024-03-01           4        9               NaN       NaN       NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Add the columns discussed above with appropriate values here\"\"\"\n",
    "# BEGIN - YOUR CODE GOES HERE\n",
    "df_transactions['TX_DATE'] = pd.to_datetime(df_transactions['TX_DATE'])\n",
    "\n",
    "daily_median = df_transactions.groupby(['CUSTOMER_ID', 'TX_DATE'])['TX_AMOUNT'].median().reset_index()\n",
    "daily_median.rename(columns={'TX_AMOUNT': 'SPENT_1D'}, inplace=True)\n",
    "daily_median['SPENT_1D'] = daily_median.groupby('CUSTOMER_ID')['SPENT_1D'].shift()\n",
    "df_transactions = pd.merge(df_transactions, daily_median[['CUSTOMER_ID', 'TX_DATE', 'SPENT_1D']], \n",
    "                           on=['CUSTOMER_ID', 'TX_DATE'], \n",
    "                           how='left')\n",
    "\n",
    "weekly_median = df_transactions.groupby(['CUSTOMER_ID', 'TX_WEEK'])['TX_AMOUNT'].median().reset_index()\n",
    "weekly_median.rename(columns={'TX_AMOUNT': 'SPENT_1W'}, inplace=True)\n",
    "weekly_median['SPENT_1W'] = weekly_median.groupby('CUSTOMER_ID')['SPENT_1W'].shift()\n",
    "df_transactions = pd.merge(df_transactions, weekly_median[['CUSTOMER_ID', 'TX_WEEK', 'SPENT_1W']], \n",
    "                           on=['CUSTOMER_ID', 'TX_WEEK'], \n",
    "                           how='left')\n",
    "df_transactions.sort_values(by= \"TX_DATETIME\").head()\n",
    "# END - YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ed7f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "795551ed6c99a6d52210a7555ca3ac9a",
     "grade": false,
     "grade_id": "cell-8acdc9e5b0753104",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong> Q3 (vi) [<font color=\"red\"> 0.5 marks </font>]</strong> Generate a scatter plot with amount (in dollars) on y-axis, and customer id on the x-axis. The scatter plot should have two markers, one for the median amount a customer spent on the previous day, and second for the value of fraudulent transactions for that customer. You should label the plot appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e45239b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:27:24.394827Z",
     "start_time": "2024-08-28T13:27:23.751503Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aff8e6894bccfab329cbfe27e53d69cd",
     "grade": true,
     "grade_id": "cell-591f9f1d0dd4c995",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Generate your plot here\"\"\"\n",
    "# BEGIN - YOUR CODE GOES HERE\n",
    "fraud_transactions = df_transactions[df_transactions['TX_FRAUD'] == 1]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "# Median amount spent by customer on the previous day (SPENT_1D)\n",
    "plt.scatter(df_transactions['CUSTOMER_ID'].values, df_transactions['SPENT_1D'], \n",
    "            color='blue', alpha=0.5, label='Median spend previous day')\n",
    "\n",
    "# Fraudulent transaction amounts\n",
    "plt.scatter(fraud_transactions['CUSTOMER_ID'].values, fraud_transactions['TX_AMOUNT'], \n",
    "            color='red', alpha=0.5, label='Fraudulent transactions amounts')\n",
    "\n",
    "plt.title('Customer spending vs fraudulent transactions amounts')\n",
    "plt.xlabel('Customer ID')\n",
    "plt.ylabel('Amount ($)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# END - YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f12e7e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82efc4be02cf02f8b3feb011abd502d2",
     "grade": false,
     "grade_id": "cell-9231eb230dad50c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong> Q3 (vii) [<font color=\"red\"> 0.5 marks </font>]</strong> Will including the amount a customer spent in the previous day help in improving the logistic regression model from the previous question? You do not need to run the logistic regression model. You should answer the question in the context of the scatter plot from previous question.  [Word limit < 150 words] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0400d67",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d1073d85d4d7679432b40bac225187f",
     "grade": false,
     "grade_id": "cell-8a4d1697458dc443",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<span class=\"label label-info text-uppercase\">Note</span>\n",
    "<span>Write your justification in the Markdown cell below</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37602f6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c7877784395ad026420cc237d4d5900",
     "grade": true,
     "grade_id": "cell-821e75fc90cc6f30",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Based on the scatter plot, the median spend of customers on the previous day seems to be concentrated mainly in the lower range, with a dense clustering of points near the bottom of the plot. The amounts for fraudulent transactions are dispersed at higher values and do not show a strong overlap with the median spending. This visualization suggests that the amount spent in fraudulent transactions tends to be higher compared to the median daily spending of customers, indicating that there might be a distinct pattern where higher spending could be associated with fraud. This means adding this feature to the model will likely improve the performance of the model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab11216",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77c796682f110e05c923b1e1e33a5052",
     "grade": false,
     "grade_id": "cell-dd67d207251e7cf0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 4 [3.5 marks]\n",
    "\n",
    "This question will use the `card_data.csv` dataset. This data has been loaded for you in the `df_card_data` variable. The data contains transactions from `2024-04-01` till `2024-04-10` (inclusive). You should use all data till `2024-04-09` (inclusive) for training, and transactions that happened on `2024-04-10` for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a9ad1b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:27:24.589374Z",
     "start_time": "2024-08-28T13:27:24.396052Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e692b258b7076520abae6aea6a25aae0",
     "grade": false,
     "grade_id": "cell-516e0f4024fcb839",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TX_DATETIME</th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>TERMINAL_ID</th>\n",
       "      <th>TX_AMOUNT</th>\n",
       "      <th>TX_FRAUD</th>\n",
       "      <th>IS_WEEKEND</th>\n",
       "      <th>IS_WEDNESDAY</th>\n",
       "      <th>IS_NIGHT</th>\n",
       "      <th>TX_HOUR</th>\n",
       "      <th>TX_DATE</th>\n",
       "      <th>...</th>\n",
       "      <th>CUSTOMER_RISK_1D</th>\n",
       "      <th>TX_WEEK</th>\n",
       "      <th>TERMINAL_TOTAL_1W</th>\n",
       "      <th>TERMINAL_FRAUD_1W</th>\n",
       "      <th>TERMINAL_RISK_1W</th>\n",
       "      <th>CUSTOMER_TOTAL_1W</th>\n",
       "      <th>CUSTOMER_FRAUD_1W</th>\n",
       "      <th>CUSTOMER_RISK_1W</th>\n",
       "      <th>SPENT_1D</th>\n",
       "      <th>SPENT_1W</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRANSACTION_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1081139</th>\n",
       "      <td>2024-04-01 00:02:24</td>\n",
       "      <td>363</td>\n",
       "      <td>7797</td>\n",
       "      <td>254.71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>275.366667</td>\n",
       "      <td>189.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084811</th>\n",
       "      <td>2024-04-01 10:19:01</td>\n",
       "      <td>363</td>\n",
       "      <td>9851</td>\n",
       "      <td>176.84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>275.366667</td>\n",
       "      <td>189.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085737</th>\n",
       "      <td>2024-04-01 11:37:14</td>\n",
       "      <td>363</td>\n",
       "      <td>7701</td>\n",
       "      <td>130.33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>275.366667</td>\n",
       "      <td>189.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149170</th>\n",
       "      <td>2024-04-07 01:00:02</td>\n",
       "      <td>363</td>\n",
       "      <td>6059</td>\n",
       "      <td>188.83</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-04-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>380.720000</td>\n",
       "      <td>189.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152136</th>\n",
       "      <td>2024-04-07 07:37:41</td>\n",
       "      <td>363</td>\n",
       "      <td>769</td>\n",
       "      <td>178.29</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-04-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>380.720000</td>\n",
       "      <td>189.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TX_DATETIME  CUSTOMER_ID  TERMINAL_ID  TX_AMOUNT  \\\n",
       "TRANSACTION_ID                                                            \n",
       "1081139        2024-04-01 00:02:24          363         7797     254.71   \n",
       "1084811        2024-04-01 10:19:01          363         9851     176.84   \n",
       "1085737        2024-04-01 11:37:14          363         7701     130.33   \n",
       "1149170        2024-04-07 01:00:02          363         6059     188.83   \n",
       "1152136        2024-04-07 07:37:41          363          769     178.29   \n",
       "\n",
       "                TX_FRAUD  IS_WEEKEND  IS_WEDNESDAY  IS_NIGHT  TX_HOUR  \\\n",
       "TRANSACTION_ID                                                          \n",
       "1081139                0           0             0         1        0   \n",
       "1084811                0           0             0         0       10   \n",
       "1085737                0           0             0         0       11   \n",
       "1149170                0           1             0         1        1   \n",
       "1152136                0           1             0         0        7   \n",
       "\n",
       "                   TX_DATE  ...  CUSTOMER_RISK_1D  TX_WEEK  TERMINAL_TOTAL_1W  \\\n",
       "TRANSACTION_ID              ...                                                 \n",
       "1081139         2024-04-01  ...               0.0       14                5.0   \n",
       "1084811         2024-04-01  ...               0.0       14                2.0   \n",
       "1085737         2024-04-01  ...               0.0       14                7.0   \n",
       "1149170         2024-04-07  ...               0.0       14                6.0   \n",
       "1152136         2024-04-07  ...               0.0       14                2.0   \n",
       "\n",
       "                TERMINAL_FRAUD_1W  TERMINAL_RISK_1W  CUSTOMER_TOTAL_1W  \\\n",
       "TRANSACTION_ID                                                           \n",
       "1081139                       0.0               0.0               16.0   \n",
       "1084811                       0.0               0.0               16.0   \n",
       "1085737                       0.0               0.0               16.0   \n",
       "1149170                       0.0               0.0               16.0   \n",
       "1152136                       0.0               0.0               16.0   \n",
       "\n",
       "                CUSTOMER_FRAUD_1W  CUSTOMER_RISK_1W    SPENT_1D  SPENT_1W  \n",
       "TRANSACTION_ID                                                             \n",
       "1081139                       0.0               0.0  275.366667    189.17  \n",
       "1084811                       0.0               0.0  275.366667    189.17  \n",
       "1085737                       0.0               0.0  275.366667    189.17  \n",
       "1149170                       0.0               0.0  380.720000    189.17  \n",
       "1152136                       0.0               0.0  380.720000    189.17  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Use the preloaded data in your workings below;\n",
    "This is a Read-Only cell. Remember to execute this cell once\"\"\"\n",
    "df_card_data = pd.read_csv(\"./card_data.csv\", index_col=\"TRANSACTION_ID\", parse_dates=[\"TX_DATETIME\"])\n",
    "df_card_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894c1af8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc1b7b420a5fa048334a8bce8e72100d",
     "grade": false,
     "grade_id": "cell-aea91b5aee73ce78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong> Q4 (i) [<font color=\"red\"> 0.5 marks </font>]</strong> Train a logistic regression model using the relevant features from the training portion of the card data. You should use the given variable to store the fitted logistic regression model. You should also use the `statsmodels` package for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd8d2c7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:27:24.592361Z",
     "start_time": "2024-08-28T13:27:24.590499Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78a63eaaed70e6268df3a28f26837e0a",
     "grade": false,
     "grade_id": "cell-bd473adf534813cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Predefined Variables - Do Not Change their Name;\n",
    "This is a Read-Only cell. Remember to execute this cell once\"\"\"\n",
    "logit_2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c0e9045",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T14:03:51.881233Z",
     "start_time": "2024-08-28T14:03:49.971079Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf027dfcc5b047c40db5863182374c7d",
     "grade": true,
     "grade_id": "cell-ca71f6b0413f8fce",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.014599\n",
      "         Iterations 11\n",
      "0.797\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>TX_FRAUD</td>     <th>  No. Observations:  </th>  <td>102658</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>102637</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    20</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 29 Aug 2024</td> <th>  Pseudo R-squ.:     </th>  <td>0.4699</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>00:03:51</td>     <th>  Log-Likelihood:    </th> <td> -1498.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -2827.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TX_AMOUNT</th>         <td>    0.0137</td> <td>    0.001</td> <td>   23.105</td> <td> 0.000</td> <td>    0.013</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>IS_WEEKEND</th>        <td>   -0.7024</td> <td>    0.160</td> <td>   -4.381</td> <td> 0.000</td> <td>   -1.017</td> <td>   -0.388</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>IS_WEDNESDAY</th>      <td>    0.1666</td> <td>    0.188</td> <td>    0.887</td> <td> 0.375</td> <td>   -0.201</td> <td>    0.535</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>IS_NIGHT</th>          <td>    0.0467</td> <td>    0.254</td> <td>    0.183</td> <td> 0.855</td> <td>   -0.452</td> <td>    0.545</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TX_HOUR</th>           <td>    0.0018</td> <td>    0.015</td> <td>    0.125</td> <td> 0.900</td> <td>   -0.027</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TERMINAL_TOTAL_1D</th> <td>   -0.0163</td> <td>    0.072</td> <td>   -0.226</td> <td> 0.821</td> <td>   -0.158</td> <td>    0.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TERMINAL_FRAUD_1D</th> <td>    1.6687</td> <td>    0.231</td> <td>    7.218</td> <td> 0.000</td> <td>    1.216</td> <td>    2.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TERMINAL_RISK_1D</th>  <td>    4.0095</td> <td>    0.362</td> <td>   11.061</td> <td> 0.000</td> <td>    3.299</td> <td>    4.720</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CUSTOMER_TOTAL_1D</th> <td>    0.0070</td> <td>    0.029</td> <td>    0.246</td> <td> 0.806</td> <td>   -0.049</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CUSTOMER_FRAUD_1D</th> <td>    1.0058</td> <td>    0.177</td> <td>    5.677</td> <td> 0.000</td> <td>    0.659</td> <td>    1.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CUSTOMER_RISK_1D</th>  <td>    1.8200</td> <td>    0.630</td> <td>    2.890</td> <td> 0.004</td> <td>    0.586</td> <td>    3.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TX_WEEK</th>           <td>   -0.2291</td> <td>    0.187</td> <td>   -1.227</td> <td> 0.220</td> <td>   -0.595</td> <td>    0.137</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TERMINAL_TOTAL_1W</th> <td>    0.0034</td> <td>    0.018</td> <td>    0.186</td> <td> 0.852</td> <td>   -0.032</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TERMINAL_FRAUD_1W</th> <td>    0.2114</td> <td>    0.109</td> <td>    1.945</td> <td> 0.052</td> <td>   -0.002</td> <td>    0.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TERMINAL_RISK_1W</th>  <td>   -0.4245</td> <td>    0.846</td> <td>   -0.502</td> <td> 0.616</td> <td>   -2.082</td> <td>    1.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CUSTOMER_TOTAL_1W</th> <td>    0.0026</td> <td>    0.007</td> <td>    0.385</td> <td> 0.700</td> <td>   -0.011</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CUSTOMER_FRAUD_1W</th> <td>    0.3981</td> <td>    0.089</td> <td>    4.497</td> <td> 0.000</td> <td>    0.225</td> <td>    0.572</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CUSTOMER_RISK_1W</th>  <td>    2.3738</td> <td>    2.005</td> <td>    1.184</td> <td> 0.236</td> <td>   -1.556</td> <td>    6.304</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SPENT_1D</th>          <td>    0.0006</td> <td>    0.001</td> <td>    0.635</td> <td> 0.525</td> <td>   -0.001</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SPENT_1W</th>          <td>   -0.0191</td> <td>    0.002</td> <td>  -12.057</td> <td> 0.000</td> <td>   -0.022</td> <td>   -0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>             <td>   -3.0439</td> <td>    2.690</td> <td>   -1.131</td> <td> 0.258</td> <td>   -8.317</td> <td>    2.229</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:               TX_FRAUD   No. Observations:               102658\n",
       "Model:                          Logit   Df Residuals:                   102637\n",
       "Method:                           MLE   Df Model:                           20\n",
       "Date:                Thu, 29 Aug 2024   Pseudo R-squ.:                  0.4699\n",
       "Time:                        00:03:51   Log-Likelihood:                -1498.8\n",
       "converged:                       True   LL-Null:                       -2827.2\n",
       "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
       "=====================================================================================\n",
       "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "TX_AMOUNT             0.0137      0.001     23.105      0.000       0.013       0.015\n",
       "IS_WEEKEND           -0.7024      0.160     -4.381      0.000      -1.017      -0.388\n",
       "IS_WEDNESDAY          0.1666      0.188      0.887      0.375      -0.201       0.535\n",
       "IS_NIGHT              0.0467      0.254      0.183      0.855      -0.452       0.545\n",
       "TX_HOUR               0.0018      0.015      0.125      0.900      -0.027       0.030\n",
       "TERMINAL_TOTAL_1D    -0.0163      0.072     -0.226      0.821      -0.158       0.125\n",
       "TERMINAL_FRAUD_1D     1.6687      0.231      7.218      0.000       1.216       2.122\n",
       "TERMINAL_RISK_1D      4.0095      0.362     11.061      0.000       3.299       4.720\n",
       "CUSTOMER_TOTAL_1D     0.0070      0.029      0.246      0.806      -0.049       0.063\n",
       "CUSTOMER_FRAUD_1D     1.0058      0.177      5.677      0.000       0.659       1.353\n",
       "CUSTOMER_RISK_1D      1.8200      0.630      2.890      0.004       0.586       3.054\n",
       "TX_WEEK              -0.2291      0.187     -1.227      0.220      -0.595       0.137\n",
       "TERMINAL_TOTAL_1W     0.0034      0.018      0.186      0.852      -0.032       0.039\n",
       "TERMINAL_FRAUD_1W     0.2114      0.109      1.945      0.052      -0.002       0.424\n",
       "TERMINAL_RISK_1W     -0.4245      0.846     -0.502      0.616      -2.082       1.233\n",
       "CUSTOMER_TOTAL_1W     0.0026      0.007      0.385      0.700      -0.011       0.016\n",
       "CUSTOMER_FRAUD_1W     0.3981      0.089      4.497      0.000       0.225       0.572\n",
       "CUSTOMER_RISK_1W      2.3738      2.005      1.184      0.236      -1.556       6.304\n",
       "SPENT_1D              0.0006      0.001      0.635      0.525      -0.001       0.003\n",
       "SPENT_1W             -0.0191      0.002    -12.057      0.000      -0.022      -0.016\n",
       "const                -3.0439      2.690     -1.131      0.258      -8.317       2.229\n",
       "=====================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Populate the variables shown above with appropriate values here\"\"\"\n",
    "# BEGIN - YOUR CODE GOES HERE\n",
    "df_card_data[\"TX_DATE\"] = pd.to_datetime(df_card_data[\"TX_DATE\"] )\n",
    "df_card_train = df_card_data.loc[df_card_data['TX_DATE'] <= pd.to_datetime(\"2024-04-09\")]\n",
    "df_card_test = df_card_data.loc[df_card_data['TX_DATE'] == pd.to_datetime(\"2024-04-10\")]\n",
    "\n",
    "df_card_train = sm.add_constant(df_card_train)\n",
    "features = ['TX_AMOUNT','IS_WEEKEND', 'IS_WEDNESDAY', 'IS_NIGHT', 'TX_HOUR',\n",
    "       'TERMINAL_TOTAL_1D', 'TERMINAL_FRAUD_1D', 'TERMINAL_RISK_1D',\n",
    "       'CUSTOMER_TOTAL_1D', 'CUSTOMER_FRAUD_1D', 'CUSTOMER_RISK_1D', 'TX_WEEK',\n",
    "       'TERMINAL_TOTAL_1W', 'TERMINAL_FRAUD_1W', 'TERMINAL_RISK_1W',\n",
    "       'CUSTOMER_TOTAL_1W', 'CUSTOMER_FRAUD_1W', 'CUSTOMER_RISK_1W',\n",
    "       'SPENT_1D', 'SPENT_1W', 'const']\n",
    "predict = \"TX_FRAUD\"\n",
    "\n",
    "x = df_card_train[features]\n",
    "y = df_card_train[predict]\n",
    "\n",
    "df_card_test['const'] = 1.0\n",
    "logit_2 = sm.Logit(y,x).fit()\n",
    "logit_2_probs = logit_2.predict(df_card_test[features]) \n",
    "logit_2_auc = roc_auc_score(df_card_test[predict], logit_2_probs)\n",
    "print(round(logit_2_auc,3))\n",
    "logit_2.summary()\n",
    "# END - YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e01bdc8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:27:26.282524Z",
     "start_time": "2024-08-28T13:27:26.279083Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e881b2ba0080038f5ff95897c059547b",
     "grade": false,
     "grade_id": "cell-78fc6530b40e4bc0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"This is a Read-Only cell. Remember to execute this cell once\"\"\"\n",
    "try:\n",
    "    logit_2.summary()\n",
    "except NameError:\n",
    "    print(f\"Did you forget to run the readonly cell above?\")\n",
    "except AttributeError:\n",
    "    print(f\"Your code is possibly incorrect for creating logit_2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357ee11b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d8597997992275f99f7fbf380d69ddd",
     "grade": false,
     "grade_id": "cell-b113c406319e207e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong> Q4 (ii) [<font color=\"red\"> 1 marks </font>]</strong> You will find that a number of features that model information related to the previous week are statistically insignificant at $5\\%$. Explain possible reasons behind this finding in the context of fraud detection.  [Word limit < 150 words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b73ce75",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "203b4480e5f80e7e396dc048a7d301dc",
     "grade": false,
     "grade_id": "cell-66d5271b955c4767",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<span class=\"label label-info text-uppercase\">Note</span>\n",
    "<span>Write your explanation in the Markdown cell below</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395b612",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ebdc72f56d6d485e86847467c4e29cf",
     "grade": true,
     "grade_id": "cell-b065e4b222b5e5a2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The statistical insignificance of several features related to the previous week in the context of fraud detection could be due to several factors. Fraudulent behavior often deviates significantly from a customer's typical transaction patterns. Therefore, data from a week prior may not be as relevant in detecting current fraud, especially since fraud tends to be more spontaneous and less predictable. Additionally, frauds tend to adapt quickly to detection methods, making historical patterns less effective in identifying new fraudulent transactions. Furthermore, if the previous week's data is not well-correlated with current fraudulent activities, it would naturally be less significant in the model. Lastly, other variables like same-day or more recent transaction data might be more indicative of fraud, overshadowing the importance of older data. Thus, features related to the previous week may not significantly contribute to the model's predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59116a10",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "56174b5b51499516facf286198fb5b88",
     "grade": false,
     "grade_id": "cell-8ec900be58905760",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong> Q4 (iii) [<font color=\"red\"> 0.5 marks </font>]</strong> Complete the `build_nn_model()` function that takes six arguments:\n",
    "1. The input features to be used to train the neural network\n",
    "2. The feature to predict\n",
    "3. The number of hidden layers in the neural network\n",
    "4. The number of units per layer\n",
    "5. The pandas dataframe containing input and output features\n",
    "6. The date to split the given data into training and test sets. All the data before the given date will be used for training, and the data from the given date onwards will be used for testing.\n",
    "\n",
    "The function should return the trained neural network and the auc for training data, auc for testing data. You should use sklearn metrics to compute AUC scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "990054e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:27:26.306228Z",
     "start_time": "2024-08-28T13:27:26.283485Z"
    },
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84d5cd46163a50ceb6c128e521014bec",
     "grade": true,
     "grade_id": "cell-5f7fd1b47cd9253b",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Populate the variables shown above with appropriate values here\"\"\"\n",
    "# BEGIN - YOUR CODE GOES HERE\n",
    "def build_nn_model(input_features, output_feature, num_layers, num_units, data, split_date):\n",
    "    clear_session()  \n",
    "    train_data = data[data[\"TX_DATE\"] < pd.to_datetime(split_date)]\n",
    "    test_data = data[data[\"TX_DATE\"] >= pd.to_datetime(split_date)]\n",
    "    \n",
    "    X_train = train_data[input_features]\n",
    "    y_train = train_data[output_feature]\n",
    "    X_test = test_data[input_features]\n",
    "    y_test = test_data[output_feature]\n",
    "\n",
    "    input_shape = X_train.shape[1:]\n",
    "    input_layer = Input(shape=input_shape, name=\"input_layer\")\n",
    "    op = input_layer\n",
    "    for i in range(1, num_layers + 1):\n",
    "        op = Dense(units=num_units, activation=\"relu\", name=f\"hidden_layer_{i}\")(op)\n",
    "    output_layer = Dense(units=1, activation=\"sigmoid\", name=\"output_layer\")(op)\n",
    "    \n",
    "    nn = Model(inputs=input_layer, outputs=output_layer)\n",
    "    nn.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Set training parameters\n",
    "    epochs = 1000\n",
    "    patience = 2\n",
    "    verbose = 2\n",
    "    batch_size = X_train.shape[0]\n",
    "\n",
    "    # Define the early stopping callback\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=patience)]\n",
    "\n",
    "    # Train and validate the model\n",
    "    nn.fit(X_train, y_train,\n",
    "           epochs=epochs,\n",
    "           callbacks=callbacks,\n",
    "           validation_data=(X_test, y_test),\n",
    "           batch_size=batch_size,\n",
    "           verbose=verbose)\n",
    "    \n",
    "    # Compute AUC for train and test sets\n",
    "    train_auc = roc_auc_score(y_train, nn.predict(X_train).ravel())\n",
    "    test_auc = roc_auc_score(y_test, nn.predict(X_test).ravel())\n",
    "    return nn, train_auc, test_auc\n",
    "# END - YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad4e70",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d70d3597c028baaf63e46623beb54409",
     "grade": false,
     "grade_id": "cell-ffb88a092e9ea5ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong> Q4 (iv) [<font color=\"red\"> 0.5 marks </font>]</strong> Build two neural network models with 4 hidden layers and 16 units, one with a split date of `2024-04-03`, and another with a split date of `2024-04-09`. You are free to use any input features. You should use the function `build_nn_model()` from the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a54ea0fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:27:37.029815Z",
     "start_time": "2024-08-28T13:27:26.309468Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7da2f417f1e86da5dfcef4bd7c7eec5a",
     "grade": true,
     "grade_id": "cell-802bf9b1f92c3215",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 - 1s - loss: 8.5075 - accuracy: 0.0043 - val_loss: 7.1433 - val_accuracy: 0.0056 - 596ms/epoch - 596ms/step\n",
      "Epoch 2/1000\n",
      "1/1 - 0s - loss: 7.1087 - accuracy: 0.0050 - val_loss: 5.8819 - val_accuracy: 0.0096 - 37ms/epoch - 37ms/step\n",
      "Epoch 3/1000\n",
      "1/1 - 0s - loss: 5.8429 - accuracy: 0.0091 - val_loss: 4.8020 - val_accuracy: 0.0183 - 38ms/epoch - 38ms/step\n",
      "Epoch 4/1000\n",
      "1/1 - 0s - loss: 4.7491 - accuracy: 0.0178 - val_loss: 3.9090 - val_accuracy: 0.0328 - 57ms/epoch - 57ms/step\n",
      "Epoch 5/1000\n",
      "1/1 - 0s - loss: 3.8440 - accuracy: 0.0326 - val_loss: 3.1875 - val_accuracy: 0.0608 - 34ms/epoch - 34ms/step\n",
      "Epoch 6/1000\n",
      "1/1 - 0s - loss: 3.1194 - accuracy: 0.0644 - val_loss: 2.5908 - val_accuracy: 0.1157 - 63ms/epoch - 63ms/step\n",
      "Epoch 7/1000\n",
      "1/1 - 0s - loss: 2.5313 - accuracy: 0.1212 - val_loss: 2.0830 - val_accuracy: 0.1731 - 39ms/epoch - 39ms/step\n",
      "Epoch 8/1000\n",
      "1/1 - 0s - loss: 2.0398 - accuracy: 0.1783 - val_loss: 1.6410 - val_accuracy: 0.2318 - 46ms/epoch - 46ms/step\n",
      "Epoch 9/1000\n",
      "1/1 - 0s - loss: 1.6165 - accuracy: 0.2370 - val_loss: 1.2616 - val_accuracy: 0.3171 - 38ms/epoch - 38ms/step\n",
      "Epoch 10/1000\n",
      "1/1 - 0s - loss: 1.2523 - accuracy: 0.3114 - val_loss: 0.9504 - val_accuracy: 0.4393 - 57ms/epoch - 57ms/step\n",
      "Epoch 11/1000\n",
      "1/1 - 0s - loss: 0.9493 - accuracy: 0.4266 - val_loss: 0.7099 - val_accuracy: 0.5806 - 36ms/epoch - 36ms/step\n",
      "Epoch 12/1000\n",
      "1/1 - 0s - loss: 0.7076 - accuracy: 0.5798 - val_loss: 0.5315 - val_accuracy: 0.7039 - 60ms/epoch - 60ms/step\n",
      "Epoch 13/1000\n",
      "1/1 - 0s - loss: 0.5235 - accuracy: 0.7137 - val_loss: 0.3984 - val_accuracy: 0.7910 - 36ms/epoch - 36ms/step\n",
      "Epoch 14/1000\n",
      "1/1 - 0s - loss: 0.3844 - accuracy: 0.8108 - val_loss: 0.2972 - val_accuracy: 0.8541 - 52ms/epoch - 52ms/step\n",
      "Epoch 15/1000\n",
      "1/1 - 0s - loss: 0.2788 - accuracy: 0.8777 - val_loss: 0.2219 - val_accuracy: 0.9058 - 33ms/epoch - 33ms/step\n",
      "Epoch 16/1000\n",
      "1/1 - 0s - loss: 0.2016 - accuracy: 0.9268 - val_loss: 0.1647 - val_accuracy: 0.9466 - 59ms/epoch - 59ms/step\n",
      "Epoch 17/1000\n",
      "1/1 - 0s - loss: 0.1473 - accuracy: 0.9618 - val_loss: 0.1218 - val_accuracy: 0.9764 - 34ms/epoch - 34ms/step\n",
      "Epoch 18/1000\n",
      "1/1 - 0s - loss: 0.1092 - accuracy: 0.9834 - val_loss: 0.0927 - val_accuracy: 0.9910 - 38ms/epoch - 38ms/step\n",
      "Epoch 19/1000\n",
      "1/1 - 0s - loss: 0.0838 - accuracy: 0.9932 - val_loss: 0.0740 - val_accuracy: 0.9944 - 56ms/epoch - 56ms/step\n",
      "Epoch 20/1000\n",
      "1/1 - 0s - loss: 0.0677 - accuracy: 0.9955 - val_loss: 0.0625 - val_accuracy: 0.9950 - 35ms/epoch - 35ms/step\n",
      "Epoch 21/1000\n",
      "1/1 - 0s - loss: 0.0579 - accuracy: 0.9959 - val_loss: 0.0558 - val_accuracy: 0.9954 - 56ms/epoch - 56ms/step\n",
      "Epoch 22/1000\n",
      "1/1 - 0s - loss: 0.0520 - accuracy: 0.9958 - val_loss: 0.0520 - val_accuracy: 0.9954 - 36ms/epoch - 36ms/step\n",
      "Epoch 23/1000\n",
      "1/1 - 0s - loss: 0.0488 - accuracy: 0.9958 - val_loss: 0.0500 - val_accuracy: 0.9955 - 48ms/epoch - 48ms/step\n",
      "Epoch 24/1000\n",
      "1/1 - 0s - loss: 0.0471 - accuracy: 0.9959 - val_loss: 0.0493 - val_accuracy: 0.9955 - 37ms/epoch - 37ms/step\n",
      "Epoch 25/1000\n",
      "1/1 - 0s - loss: 0.0465 - accuracy: 0.9959 - val_loss: 0.0493 - val_accuracy: 0.9955 - 55ms/epoch - 55ms/step\n",
      "Epoch 26/1000\n",
      "1/1 - 0s - loss: 0.0465 - accuracy: 0.9959 - val_loss: 0.0497 - val_accuracy: 0.9955 - 33ms/epoch - 33ms/step\n",
      "608/608 [==============================] - 0s 563us/step\n",
      "3024/3024 [==============================] - 2s 545us/step\n",
      "Epoch 1/1000\n",
      "1/1 - 1s - loss: 0.1365 - accuracy: 0.9955 - val_loss: 0.1086 - val_accuracy: 0.9957 - 677ms/epoch - 677ms/step\n",
      "Epoch 2/1000\n",
      "1/1 - 0s - loss: 0.1151 - accuracy: 0.9956 - val_loss: 0.0938 - val_accuracy: 0.9957 - 30ms/epoch - 30ms/step\n",
      "Epoch 3/1000\n",
      "1/1 - 0s - loss: 0.1036 - accuracy: 0.9956 - val_loss: 0.0849 - val_accuracy: 0.9957 - 51ms/epoch - 51ms/step\n",
      "Epoch 4/1000\n",
      "1/1 - 0s - loss: 0.0967 - accuracy: 0.9956 - val_loss: 0.0791 - val_accuracy: 0.9957 - 27ms/epoch - 27ms/step\n",
      "Epoch 5/1000\n",
      "1/1 - 0s - loss: 0.0921 - accuracy: 0.9956 - val_loss: 0.0749 - val_accuracy: 0.9957 - 71ms/epoch - 71ms/step\n",
      "Epoch 6/1000\n",
      "1/1 - 0s - loss: 0.0887 - accuracy: 0.9956 - val_loss: 0.0717 - val_accuracy: 0.9957 - 28ms/epoch - 28ms/step\n",
      "Epoch 7/1000\n",
      "1/1 - 0s - loss: 0.0859 - accuracy: 0.9956 - val_loss: 0.0691 - val_accuracy: 0.9957 - 69ms/epoch - 69ms/step\n",
      "Epoch 8/1000\n",
      "1/1 - 0s - loss: 0.0834 - accuracy: 0.9956 - val_loss: 0.0667 - val_accuracy: 0.9957 - 28ms/epoch - 28ms/step\n",
      "Epoch 9/1000\n",
      "1/1 - 0s - loss: 0.0810 - accuracy: 0.9956 - val_loss: 0.0646 - val_accuracy: 0.9957 - 70ms/epoch - 70ms/step\n",
      "Epoch 10/1000\n",
      "1/1 - 0s - loss: 0.0787 - accuracy: 0.9956 - val_loss: 0.0626 - val_accuracy: 0.9957 - 74ms/epoch - 74ms/step\n",
      "Epoch 11/1000\n",
      "1/1 - 0s - loss: 0.0764 - accuracy: 0.9956 - val_loss: 0.0607 - val_accuracy: 0.9957 - 27ms/epoch - 27ms/step\n",
      "Epoch 12/1000\n",
      "1/1 - 0s - loss: 0.0741 - accuracy: 0.9956 - val_loss: 0.0588 - val_accuracy: 0.9957 - 75ms/epoch - 75ms/step\n",
      "Epoch 13/1000\n",
      "1/1 - 0s - loss: 0.0717 - accuracy: 0.9956 - val_loss: 0.0569 - val_accuracy: 0.9957 - 27ms/epoch - 27ms/step\n",
      "Epoch 14/1000\n",
      "1/1 - 0s - loss: 0.0692 - accuracy: 0.9956 - val_loss: 0.0551 - val_accuracy: 0.9957 - 82ms/epoch - 82ms/step\n",
      "Epoch 15/1000\n",
      "1/1 - 0s - loss: 0.0667 - accuracy: 0.9956 - val_loss: 0.0533 - val_accuracy: 0.9957 - 26ms/epoch - 26ms/step\n",
      "Epoch 16/1000\n",
      "1/1 - 0s - loss: 0.0642 - accuracy: 0.9956 - val_loss: 0.0514 - val_accuracy: 0.9957 - 72ms/epoch - 72ms/step\n",
      "Epoch 17/1000\n",
      "1/1 - 0s - loss: 0.0617 - accuracy: 0.9956 - val_loss: 0.0496 - val_accuracy: 0.9957 - 27ms/epoch - 27ms/step\n",
      "Epoch 18/1000\n",
      "1/1 - 0s - loss: 0.0592 - accuracy: 0.9956 - val_loss: 0.0479 - val_accuracy: 0.9957 - 71ms/epoch - 71ms/step\n",
      "Epoch 19/1000\n",
      "1/1 - 0s - loss: 0.0567 - accuracy: 0.9956 - val_loss: 0.0463 - val_accuracy: 0.9957 - 28ms/epoch - 28ms/step\n",
      "Epoch 20/1000\n",
      "1/1 - 0s - loss: 0.0544 - accuracy: 0.9956 - val_loss: 0.0450 - val_accuracy: 0.9956 - 71ms/epoch - 71ms/step\n",
      "Epoch 21/1000\n",
      "1/1 - 0s - loss: 0.0523 - accuracy: 0.9954 - val_loss: 0.0441 - val_accuracy: 0.9959 - 27ms/epoch - 27ms/step\n",
      "Epoch 22/1000\n",
      "1/1 - 0s - loss: 0.0508 - accuracy: 0.9956 - val_loss: 0.0435 - val_accuracy: 0.9960 - 72ms/epoch - 72ms/step\n",
      "Epoch 23/1000\n",
      "1/1 - 0s - loss: 0.0497 - accuracy: 0.9956 - val_loss: 0.0431 - val_accuracy: 0.9960 - 27ms/epoch - 27ms/step\n",
      "Epoch 24/1000\n",
      "1/1 - 0s - loss: 0.0489 - accuracy: 0.9956 - val_loss: 0.0427 - val_accuracy: 0.9962 - 73ms/epoch - 73ms/step\n",
      "Epoch 25/1000\n",
      "1/1 - 0s - loss: 0.0483 - accuracy: 0.9956 - val_loss: 0.0422 - val_accuracy: 0.9961 - 75ms/epoch - 75ms/step\n",
      "Epoch 26/1000\n",
      "1/1 - 0s - loss: 0.0476 - accuracy: 0.9956 - val_loss: 0.0417 - val_accuracy: 0.9960 - 27ms/epoch - 27ms/step\n",
      "Epoch 27/1000\n",
      "1/1 - 0s - loss: 0.0468 - accuracy: 0.9956 - val_loss: 0.0410 - val_accuracy: 0.9960 - 71ms/epoch - 71ms/step\n",
      "Epoch 28/1000\n",
      "1/1 - 0s - loss: 0.0459 - accuracy: 0.9957 - val_loss: 0.0403 - val_accuracy: 0.9960 - 26ms/epoch - 26ms/step\n",
      "Epoch 29/1000\n",
      "1/1 - 0s - loss: 0.0448 - accuracy: 0.9957 - val_loss: 0.0396 - val_accuracy: 0.9960 - 72ms/epoch - 72ms/step\n",
      "Epoch 30/1000\n",
      "1/1 - 0s - loss: 0.0436 - accuracy: 0.9957 - val_loss: 0.0388 - val_accuracy: 0.9960 - 26ms/epoch - 26ms/step\n",
      "Epoch 31/1000\n",
      "1/1 - 0s - loss: 0.0426 - accuracy: 0.9957 - val_loss: 0.0382 - val_accuracy: 0.9960 - 80ms/epoch - 80ms/step\n",
      "Epoch 32/1000\n",
      "1/1 - 0s - loss: 0.0416 - accuracy: 0.9958 - val_loss: 0.0377 - val_accuracy: 0.9960 - 27ms/epoch - 27ms/step\n",
      "Epoch 33/1000\n",
      "1/1 - 0s - loss: 0.0407 - accuracy: 0.9958 - val_loss: 0.0373 - val_accuracy: 0.9959 - 78ms/epoch - 78ms/step\n",
      "Epoch 34/1000\n",
      "1/1 - 0s - loss: 0.0400 - accuracy: 0.9958 - val_loss: 0.0370 - val_accuracy: 0.9959 - 26ms/epoch - 26ms/step\n",
      "Epoch 35/1000\n",
      "1/1 - 0s - loss: 0.0394 - accuracy: 0.9958 - val_loss: 0.0368 - val_accuracy: 0.9959 - 71ms/epoch - 71ms/step\n",
      "Epoch 36/1000\n",
      "1/1 - 0s - loss: 0.0387 - accuracy: 0.9959 - val_loss: 0.0366 - val_accuracy: 0.9959 - 26ms/epoch - 26ms/step\n",
      "Epoch 37/1000\n",
      "1/1 - 0s - loss: 0.0381 - accuracy: 0.9958 - val_loss: 0.0363 - val_accuracy: 0.9959 - 72ms/epoch - 72ms/step\n",
      "Epoch 38/1000\n",
      "1/1 - 0s - loss: 0.0374 - accuracy: 0.9958 - val_loss: 0.0360 - val_accuracy: 0.9959 - 27ms/epoch - 27ms/step\n",
      "Epoch 39/1000\n",
      "1/1 - 0s - loss: 0.0367 - accuracy: 0.9958 - val_loss: 0.0356 - val_accuracy: 0.9959 - 75ms/epoch - 75ms/step\n",
      "Epoch 40/1000\n",
      "1/1 - 0s - loss: 0.0360 - accuracy: 0.9959 - val_loss: 0.0352 - val_accuracy: 0.9959 - 26ms/epoch - 26ms/step\n",
      "Epoch 41/1000\n",
      "1/1 - 0s - loss: 0.0354 - accuracy: 0.9959 - val_loss: 0.0349 - val_accuracy: 0.9959 - 68ms/epoch - 68ms/step\n",
      "Epoch 42/1000\n",
      "1/1 - 0s - loss: 0.0348 - accuracy: 0.9959 - val_loss: 0.0346 - val_accuracy: 0.9960 - 32ms/epoch - 32ms/step\n",
      "Epoch 43/1000\n",
      "1/1 - 0s - loss: 0.0344 - accuracy: 0.9959 - val_loss: 0.0344 - val_accuracy: 0.9960 - 67ms/epoch - 67ms/step\n",
      "Epoch 44/1000\n",
      "1/1 - 0s - loss: 0.0340 - accuracy: 0.9959 - val_loss: 0.0343 - val_accuracy: 0.9960 - 27ms/epoch - 27ms/step\n",
      "Epoch 45/1000\n",
      "1/1 - 0s - loss: 0.0337 - accuracy: 0.9959 - val_loss: 0.0341 - val_accuracy: 0.9960 - 71ms/epoch - 71ms/step\n",
      "Epoch 46/1000\n",
      "1/1 - 0s - loss: 0.0334 - accuracy: 0.9959 - val_loss: 0.0340 - val_accuracy: 0.9960 - 27ms/epoch - 27ms/step\n",
      "Epoch 47/1000\n",
      "1/1 - 0s - loss: 0.0330 - accuracy: 0.9959 - val_loss: 0.0340 - val_accuracy: 0.9960 - 70ms/epoch - 70ms/step\n",
      "Epoch 48/1000\n",
      "1/1 - 0s - loss: 0.0326 - accuracy: 0.9959 - val_loss: 0.0341 - val_accuracy: 0.9959 - 27ms/epoch - 27ms/step\n",
      "Epoch 49/1000\n",
      "1/1 - 0s - loss: 0.0323 - accuracy: 0.9960 - val_loss: 0.0342 - val_accuracy: 0.9959 - 71ms/epoch - 71ms/step\n",
      "2905/2905 [==============================] - 2s 571us/step\n",
      "727/727 [==============================] - 0s 583us/step\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Build your NN models here\"\"\"\n",
    "# BEGIN - YOUR CODE GOES HERE\n",
    "input_features = ['TX_AMOUNT','IS_WEEKEND', 'IS_WEDNESDAY', 'IS_NIGHT', 'TX_HOUR',\n",
    "       'TERMINAL_TOTAL_1D', 'TERMINAL_FRAUD_1D', 'TERMINAL_RISK_1D',\n",
    "       'CUSTOMER_TOTAL_1D', 'CUSTOMER_FRAUD_1D', 'CUSTOMER_RISK_1D', 'TX_WEEK',\n",
    "       'TERMINAL_TOTAL_1W', 'TERMINAL_FRAUD_1W', 'TERMINAL_RISK_1W',\n",
    "       'CUSTOMER_TOTAL_1W', 'CUSTOMER_FRAUD_1W', 'CUSTOMER_RISK_1W',\n",
    "       'SPENT_1D', 'SPENT_1W']\n",
    "output_feature = \"TX_FRAUD\"\n",
    "nn_3 = build_nn_model(input_features=input_features, output_feature=output_feature, num_layers=4, num_units=16, \n",
    "                      data=df_card_data, split_date=\"2024-04-03\")\n",
    "nn_4 = build_nn_model(input_features=input_features, output_feature=output_feature, num_layers=4, num_units=16, \n",
    "                      data=df_card_data, split_date=\"2024-04-09\")\n",
    "#print(f\"{nn_3[1]} \\n{nn_3[2]}\") 0.517 0.579\n",
    "#print(f\"{nn_4[1]} \\n{nn_4[2]}\") 0.637 0.575\n",
    "\n",
    "# END - YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49a1b2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eea7332688ac109a6ddd62ec9f379d2f",
     "grade": false,
     "grade_id": "cell-907e2b518ad9e0a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong> Q4 (v) [<font color=\"red\"> 1 marks </font>]</strong> Compare and contrast the results from the logistic regression and the neural networks. [Word limit < 150 words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f67136",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6269cae8e9cbc909ab2e874e705c9112",
     "grade": false,
     "grade_id": "cell-28fd995519cb4f17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<span class=\"label label-info text-uppercase\">Note</span>\n",
    "<span>Write your explanation in the Markdown cell below</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc08c80",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "016ecfe2d7901c0afc04ad3838e9bda2",
     "grade": true,
     "grade_id": "cell-88f5f931937c4c95",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The logistic regression model outperformed both neural networks, achieving a higher AUC score of 0.797 on the test set compared to 0.579 for nn model 3 and 0.575 for nn model 4. This suggests that the logistic regression model is better at distinguishing between fraudulent and non-fraudulent transactions in this dataset.\n",
    "The neural networks struggled, with nn model 3 showing poor performance (AUC of 0.517 on train and 0.579 on test), suggesting potential underfitting. nn model 4 slightly improved on training (AUC of 0.637) but still underperformed on the test set, indicating possible overfitting or insufficient model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcbc38d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5d176f9007f2bc199dde46800e7b86d",
     "grade": false,
     "grade_id": "cell-50558ad580cf75e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 5 [3 marks]\n",
    "\n",
    "Consider the following scenario for a credit card issuer named `BestCards`.\n",
    "The average transaction amount across the credit card transactions is \\$50 per transaction. Furthermore, let us make the following simplifying assumptions:\n",
    "1. The average transaction amount and the number of transactions do not change with time.\n",
    "2. The issuer earns $5\\%$ of a genuine transaction.\n",
    "3. If a transaction is fraudulent and not caught by the model, the issuer bears the cost of refund.\n",
    "4. A transaction goes through if the model classifies it as genuine (i.e., the other components in a fraud detection system are irrelevant).\n",
    "5. Raising a false alarm causes inconvenience to the customer and loss of revenue for the issuer. The inconvenience cost is $0.2\\%$ of the transaction amount.\n",
    "6. If a transaction is rejected, then the customer does not re-try.\n",
    "\n",
    "Alice has built a new ML model to detect fraudulent transactions and wants to charge a small fee for each transaction if BestCard wishes to use her model. The confusion matrix for BestCard's model and Alice's model are as below.\n",
    "\n",
    "\n",
    "We will use the following structure for the confusion matrix:\n",
    "\n",
    "|           | $Y_0$ | $Y_1$ |\n",
    "|-----------|-----|-----|\n",
    "| $\\hat{Y_0}$ | TN  | FN  |\n",
    "| $\\hat{Y_1}$ | FP  | TP  |\n",
    "\n",
    "\n",
    "**Confusion matrix for BestCard's model:**\n",
    "\n",
    "Training data:\n",
    "$\\begin{bmatrix}19600 & 29\\\\340 & 31\\end{bmatrix}$\n",
    "\n",
    "Testing data:\n",
    "$\\begin{bmatrix}4880  & 18\\\\82 & 20\\end{bmatrix}$\n",
    "\n",
    "\n",
    "\n",
    "**Confusion matrix for Alice's model:**\n",
    "\n",
    "Training data:\n",
    "$\\begin{bmatrix}19540  & 9\\\\400 & 51\\end{bmatrix}$\n",
    "\n",
    "Testing data:\n",
    "$\\begin{bmatrix}4870 & 4\\\\92 & 34\\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f4513",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4b94653c5e7f21851000e66beed8407",
     "grade": false,
     "grade_id": "cell-0c8f08c72c943fea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong> Q5 (i) [<font color=\"red\"> 2.5 marks </font>]</strong> \n",
    "Given the above, what is the maximum transaction fee Alice can charge BestCards for using her model, such that BestCards is indifferent between (a) paying Alice for her model, or (b) continuing to use their own model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0343ce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "def34957b7c7d6c4e5a1f48cb739724f",
     "grade": false,
     "grade_id": "cell-33c52a673ece164b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<span class=\"label label-info text-uppercase\">Note</span>\n",
    "<span>Write your Code cell below</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ebef890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T13:27:37.033172Z",
     "start_time": "2024-08-28T13:27:37.031028Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fb248d8f714a7200ef1bf4d9f8918af",
     "grade": false,
     "grade_id": "cell-9ea32674d0f452cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Predefined Variables - Do Not Change their Name;\n",
    "This is a Read-Only cell. Remember to execute this cell once\"\"\"\n",
    "alice_fee = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31892984",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T11:18:15.073141Z",
     "start_time": "2024-08-31T11:18:15.068218Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06e5621f947ee4f914ff6535ed6642c5",
     "grade": true,
     "grade_id": "cell-6758b6e04225674e",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.136"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Populate the variables shown above with appropriate values here\"\"\"\n",
    "# BEGIN - YOUR CODE GOES HERE\n",
    "P = 50  # Average transaction amount\n",
    "R = 0.05  # Revenue percentage\n",
    "I = 0.002  # Inconvenience cost percentage\n",
    "TN_A = 4870  # Genuine transaction in Alice's model\n",
    "FP_A = 92  # incorrectly classified as fraudulent in Alice's model\n",
    "FN_A = 4  # incorrectly classified as genuine in Alice's model\n",
    "TN_BC = 4880 # Genuine transaction in BC's model\n",
    "FP_BC = 82 # incorrectly classified as fraudulent in BC's model\n",
    "FN_BC = 18 # incorrectly classified as genuine in BC's model\n",
    "\n",
    "revenue_alice = R * P * (TN_A)\n",
    "inconvenience_cost_alice = I * P * FP_A\n",
    "cost_of_fraud_alice = P * FN_A\n",
    "alice_m = revenue_alice - inconvenience_cost_alice - cost_of_fraud_alice\n",
    "\n",
    "revenue_bc = R * P * (TN_BC)\n",
    "inconvenience_cost_bc = I * P * FP_BC\n",
    "cost_of_fraud_bc = P * FN_BC\n",
    "bc_m = revenue_bc - inconvenience_cost_bc - cost_of_fraud_bc\n",
    "\n",
    "diff = alice_m - bc_m\n",
    "alice_fee = round((diff/(TN_A+FP_A+FN_A)),3)\n",
    "alice_fee\n",
    "# END - YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f5af4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6239ae07f88a0a16a0fea47eb4849b07",
     "grade": false,
     "grade_id": "cell-fb6ef8a8ca57152b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong> Q5 (ii) [<font color=\"red\"> 0.5 marks </font>]</strong>\n",
    "A senior executive claims that advance machine learning models should replace all the components of the fraud detection system as they lead to more accurate detection. Do you agree or disagree with the executive? Justify your answer.  [Word limit < 150 words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46534db0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d90435753104f6648bada5caf6a19e05",
     "grade": false,
     "grade_id": "cell-184c5fed5d60cf2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<span class=\"label label-info text-uppercase\">Note</span>\n",
    "<span>Write your answer in the Markdown cell below</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93051262",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "006aa21f544ca7435462d47a8a958b4e",
     "grade": true,
     "grade_id": "cell-1656f95e456e74c7",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "I disagree with the senior executive's claim that advanced machine learning models should replace all components of the fraud detection system. While ML can improve accuracy, a hybrid approach combining machine learning with traditional rule-based systems is more effective. This is because ML models excel at identifying complex patterns in large datasets, but they may struggle with edge cases or new fraud types. On the other hand, other components such as human agents can handle new fraud scenarios or regulatory requirements. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": false,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
